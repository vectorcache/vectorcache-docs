{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Vectorcache Documentation","text":"<p>Welcome to Vectorcache - the intelligent semantic caching layer for LLM applications.</p>"},{"location":"#what-is-vectorcache","title":"What is Vectorcache?","text":"<p>Vectorcache is an AI-powered caching solution that uses semantic similarity to cache and retrieve LLM responses. Instead of exact-match caching, Vectorcache understands the meaning of queries, dramatically improving cache hit rates and reducing API costs.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83c\udfaf Semantic Matching - Uses vector embeddings to match similar queries, not just identical ones</li> <li>\ud83d\udcb0 Cost Reduction - Save up to 90% on LLM API costs with intelligent caching</li> <li>\u26a1 Fast Response Times - Serve cached responses in milliseconds instead of seconds</li> <li>\ud83d\udd12 Secure &amp; Private - Your data is encrypted and isolated per project</li> <li>\ud83d\udee0 Easy Integration - Drop-in SDK for JavaScript/TypeScript and Python</li> <li>\ud83d\udcca Analytics Dashboard - Track cache performance, costs, and usage metrics</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"JavaScript/TypeScriptPythoncURL <pre><code>import { VectorcacheClient } from 'vectorcache';\n\nconst client = new VectorcacheClient({\n  apiKey: 'your_api_key',\n  baseUrl: 'https://api.vectorcache.ai'\n});\n\nconst result = await client.query({\n  prompt: 'What is machine learning?',\n  model: 'gpt-4o',\n  similarityThreshold: 0.85\n});\n\nconsole.log(`Cache hit: ${result.cache_hit}`);\nconsole.log(`Response: ${result.response}`);\n</code></pre> <pre><code>import requests\n\nheaders = {\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndata = {\n    \"prompt\": \"What is machine learning?\",\n    \"model\": \"gpt-4o\",\n    \"similarity_threshold\": 0.85\n}\n\nresponse = requests.post(\n    \"https://api.vectorcache.ai/v1/cache/query\",\n    json=data,\n    headers=headers\n)\n\nresult = response.json()\nprint(f\"Cache hit: {result['cache_hit']}\")\n</code></pre> <pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is machine learning?\",\n    \"model\": \"gpt-4o\",\n    \"similarity_threshold\": 0.85\n  }'\n</code></pre>"},{"location":"#how-it-works","title":"How It Works","text":"<ol> <li>Query Submission - Your application sends a prompt to Vectorcache</li> <li>Semantic Search - Vectorcache searches for semantically similar cached queries</li> <li>Cache Hit/Miss - Returns cached response if similarity exceeds threshold, otherwise calls your LLM</li> <li>Cost Savings - Track savings and performance in real-time dashboard</li> </ol>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> Quick Start</p> <p>Get up and running in 5 minutes</p> <p> Quick Start Guide</p> </li> <li> <p> Installation</p> <p>Install the SDK for your platform</p> <p> Installation Guide</p> </li> <li> <p> API Reference</p> <p>Complete API documentation</p> <p> API Docs</p> </li> <li> <p> FAQ</p> <p>Common questions and answers</p> <p> View FAQ</p> </li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<ul> <li>Customer Support Chatbots - Cache common questions and responses</li> <li>Educational Platforms - Reduce costs for frequently asked educational queries</li> <li>Documentation Search - Serve similar documentation queries from cache</li> <li>Content Generation - Cache similar content requests</li> <li>Data Analysis - Reuse responses for similar analytical queries</li> </ul>"},{"location":"#why-vectorcache","title":"Why Vectorcache?","text":"<p>Traditional caching only works for exact matches. If a user asks \"What is ML?\" after someone asked \"What is machine learning?\", traditional caching misses. Vectorcache understands these are the same question and serves the cached response.</p> <p>Result: 5-10x higher cache hit rates compared to traditional caching.</p>"},{"location":"#support","title":"Support","text":"<p>Need help? We're here for you:</p> <ul> <li>\ud83d\udce7 Email: support@vectorcache.com</li> <li>\ud83d\udcac Discord: Join our community</li> <li>\ud83d\udc1b Issues: GitHub Issues</li> </ul> <p>Ready to reduce your LLM costs? Get started now \u2192</p>"},{"location":"about/faq/","title":"Frequently Asked Questions","text":"<p>Common questions about Vectorcache.</p>"},{"location":"about/faq/#general","title":"General","text":""},{"location":"about/faq/#what-is-vectorcache","title":"What is Vectorcache?","text":"<p>Vectorcache is a semantic caching layer for LLM applications. Unlike traditional caching that only matches exact queries, Vectorcache uses vector embeddings to match semantically similar queries, dramatically improving cache hit rates.</p>"},{"location":"about/faq/#how-does-semantic-caching-work","title":"How does semantic caching work?","text":"<p>When you send a query: 1. Vectorcache converts your prompt to a vector embedding 2. Searches for similar cached queries using cosine similarity 3. If similarity exceeds your threshold, returns the cached response 4. Otherwise, calls your LLM and caches the result</p>"},{"location":"about/faq/#what-llm-providers-do-you-support","title":"What LLM providers do you support?","text":"<ul> <li>OpenAI (GPT-4o, GPT-4o-mini, GPT-3.5-turbo)</li> <li>Anthropic (Claude 3.5 Sonnet, Haiku, Opus)</li> <li>Google (Gemini 1.5 Pro, Flash)</li> <li>More providers coming soon!</li> </ul>"},{"location":"about/faq/#pricing-plans","title":"Pricing &amp; Plans","text":""},{"location":"about/faq/#how-much-does-vectorcache-cost","title":"How much does Vectorcache cost?","text":"<p>See our Pricing page for current plans. Free tier available for testing.</p>"},{"location":"about/faq/#is-there-a-free-trial","title":"Is there a free trial?","text":"<p>Yes! Sign up for a free account to test Vectorcache with your application.</p>"},{"location":"about/faq/#how-do-i-calculate-roi","title":"How do I calculate ROI?","text":"<pre><code>Monthly Savings = (Cache Hits \u00d7 Avg Query Cost) - Vectorcache Cost\nROI = (Monthly Savings / Vectorcache Cost) \u00d7 100%\n</code></pre> <p>See Cost Optimization for detailed calculations.</p>"},{"location":"about/faq/#technical","title":"Technical","text":""},{"location":"about/faq/#what-is-similarity-threshold","title":"What is similarity threshold?","text":"<p>The similarity threshold (0-1) determines how similar two queries must be for a cache hit. Higher values require closer matches. We recommend starting with 0.85.</p> <p>See Similarity Tuning for details.</p>"},{"location":"about/faq/#how-fast-is-vectorcache","title":"How fast is Vectorcache?","text":"<ul> <li>Cache hit: 50-150ms</li> <li>Cache miss: 1-5 seconds (includes LLM call)</li> </ul> <p>Much faster than calling an LLM directly (typically 2-5 seconds).</p>"},{"location":"about/faq/#do-you-store-my-llm-api-keys","title":"Do you store my LLM API keys?","text":"<p>Yes, your LLM API keys are encrypted at rest using AES-256 and never exposed in logs or responses. You can delete them anytime from the dashboard.</p>"},{"location":"about/faq/#can-i-use-my-own-embedding-model","title":"Can I use my own embedding model?","text":"<p>Currently, Vectorcache uses optimized embedding models. Custom embedding models coming in a future release.</p>"},{"location":"about/faq/#what-happens-if-vectorcache-is-down","title":"What happens if Vectorcache is down?","text":"<p>Implement fallback logic in your application to call your LLM directly if Vectorcache is unavailable. See Best Practices.</p>"},{"location":"about/faq/#data-privacy","title":"Data &amp; Privacy","text":""},{"location":"about/faq/#where-is-my-data-stored","title":"Where is my data stored?","text":"<p>Data is stored in secure, SOC 2 compliant data centers in the US. EU data residency coming soon.</p>"},{"location":"about/faq/#is-my-data-encrypted","title":"Is my data encrypted?","text":"<p>Yes: - In transit: TLS 1.3 - At rest: AES-256 encryption - API keys: Separately encrypted</p>"},{"location":"about/faq/#can-i-delete-my-cached-data","title":"Can I delete my cached data?","text":"<p>Yes, you can delete cache entries anytime from the dashboard or via API (coming soon).</p>"},{"location":"about/faq/#do-you-train-models-on-my-data","title":"Do you train models on my data?","text":"<p>No, we never use your data to train models.</p>"},{"location":"about/faq/#integration","title":"Integration","text":""},{"location":"about/faq/#how-long-does-integration-take","title":"How long does integration take?","text":"<p>Most developers integrate Vectorcache in under 30 minutes:</p> <ol> <li>Install SDK (1 minute)</li> <li>Add API key (2 minutes)</li> <li>Replace LLM calls (10-20 minutes)</li> <li>Test and deploy (5-10 minutes)</li> </ol>"},{"location":"about/faq/#do-i-need-to-change-my-existing-code-much","title":"Do I need to change my existing code much?","text":"<p>Minimal changes required:</p> <pre><code>// Before\nconst response = await openai.chat.completions.create({...});\n\n// After\nconst result = await vectorcache.query({\n  prompt: userMessage,\n  model: 'gpt-4o'\n});\n</code></pre>"},{"location":"about/faq/#can-i-use-vectorcache-with-streaming-responses","title":"Can I use Vectorcache with streaming responses?","text":"<p>Streaming support coming in Q2 2025.</p>"},{"location":"about/faq/#performance","title":"Performance","text":""},{"location":"about/faq/#whats-a-good-cache-hit-rate","title":"What's a good cache hit rate?","text":"<p>Depends on your use case: - Customer support: 50-70% - Educational: 60-80% - Documentation: 40-60% - General chatbot: 30-50%</p>"},{"location":"about/faq/#how-can-i-improve-my-cache-hit-rate","title":"How can I improve my cache hit rate?","text":"<ol> <li>Lower similarity threshold (0.80-0.85)</li> <li>Normalize user input</li> <li>Use context segmentation</li> <li>Group similar queries</li> </ol> <p>See Similarity Tuning.</p>"},{"location":"about/faq/#does-caching-affect-response-quality","title":"Does caching affect response quality?","text":"<p>When configured properly, no. Use higher similarity thresholds (0.90+) for use cases requiring exact matches.</p>"},{"location":"about/faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"about/faq/#why-am-i-getting-low-cache-hit-rates","title":"Why am I getting low cache hit rates?","text":"<p>Common causes: - Threshold too high (try 0.85) - Queries are too unique - Different contexts preventing matches - Not enough cached data yet</p>"},{"location":"about/faq/#why-are-my-api-calls-failing","title":"Why are my API calls failing?","text":"<p>Check: - API key is valid and active - LLM API keys configured in dashboard - Request format is correct - Not hitting rate limits</p> <p>See Error Handling.</p>"},{"location":"about/faq/#how-do-i-debug-cache-misses","title":"How do I debug cache misses?","text":"<p>Use debug mode:</p> <pre><code>const result = await client.query({\n  prompt: 'test',\n  model: 'gpt-4o',\n  includeDebug: true\n});\n\nconsole.log(result.debug);\n</code></pre>"},{"location":"about/faq/#limits","title":"Limits","text":""},{"location":"about/faq/#what-are-the-rate-limits","title":"What are the rate limits?","text":"Tier Requests/Minute Free 100 Pro 1,000 Enterprise Custom"},{"location":"about/faq/#is-there-a-query-size-limit","title":"Is there a query size limit?","text":"<p>Maximum prompt size: 32,000 characters</p>"},{"location":"about/faq/#how-many-projects-can-i-create","title":"How many projects can I create?","text":"<ul> <li>Free: 1 project</li> <li>Pro: 10 projects</li> <li>Enterprise: Unlimited</li> </ul>"},{"location":"about/faq/#migration","title":"Migration","text":""},{"location":"about/faq/#can-i-migrate-from-another-caching-solution","title":"Can I migrate from another caching solution?","text":"<p>Yes! Contact support for migration assistance.</p>"},{"location":"about/faq/#how-do-i-export-my-cached-data","title":"How do I export my cached data?","text":"<p>Data export via API or dashboard coming soon.</p>"},{"location":"about/faq/#support","title":"Support","text":""},{"location":"about/faq/#how-do-i-get-help","title":"How do I get help?","text":"<ul> <li>\ud83d\udce7 Email: support@vectorcache.com</li> <li>\ud83d\udcac Discord: Join our community</li> <li>\ud83d\udcda Docs: docs.vectorcache.ai</li> <li>\ud83d\udc1b Issues: GitHub</li> </ul>"},{"location":"about/faq/#whats-your-sla","title":"What's your SLA?","text":"<ul> <li>Free: Best effort</li> <li>Pro: 99.5% uptime</li> <li>Enterprise: 99.9% uptime with dedicated support</li> </ul>"},{"location":"about/faq/#do-you-offer-custom-solutions","title":"Do you offer custom solutions?","text":"<p>Yes! Contact sales@vectorcache.com for enterprise and custom solutions.</p>"},{"location":"about/faq/#still-have-questions","title":"Still have questions?","text":"<p>Contact our support team - we're here to help!</p>"},{"location":"about/pricing/","title":"Pricing","text":"<p>Pricing details coming soon. Currently in beta.</p>"},{"location":"about/pricing/#beta-pricing","title":"Beta Pricing","text":"<p>During beta, Vectorcache is free to use with the following limits:</p> <ul> <li>10,000 cache queries/month</li> <li>1 project</li> <li>Community support</li> <li>All LLM providers supported</li> </ul>"},{"location":"about/pricing/#planned-pricing-subject-to-change","title":"Planned Pricing (Subject to Change)","text":""},{"location":"about/pricing/#free-tier","title":"Free Tier","text":"<ul> <li>1,000 cache queries/month</li> <li>1 project</li> <li>Community support</li> <li>Perfect for testing and small projects</li> </ul>"},{"location":"about/pricing/#pro-29month","title":"Pro - $29/month","text":"<ul> <li>100,000 cache queries/month</li> <li>10 projects</li> <li>Email support</li> <li>Advanced analytics</li> <li>99.5% SLA</li> </ul>"},{"location":"about/pricing/#enterprise-custom","title":"Enterprise - Custom","text":"<ul> <li>Unlimited queries</li> <li>Unlimited projects</li> <li>Dedicated support</li> <li>Custom SLA (99.9%+)</li> <li>On-premise deployment options</li> <li>Custom integrations</li> </ul>"},{"location":"about/pricing/#cost-calculator","title":"Cost Calculator","text":"<p>Estimate your potential savings:</p> <p>Example: - 50,000 queries/month - 50% cache hit rate = 25,000 cached responses - Average LLM cost: $0.003/query - Monthly savings: 25,000 \u00d7 $0.003 = $75 - Vectorcache cost: $29 - Net savings: $46/month ($552/year)</p>"},{"location":"about/pricing/#fair-usage-policy","title":"Fair Usage Policy","text":"<p>We monitor for abuse but don't nickel-and-dime: - Burst above limits is allowed - We'll notify you before any charges - Focus on value, not penny-counting</p>"},{"location":"about/pricing/#questions","title":"Questions?","text":"<p>Contact sales@vectorcache.com for: - Enterprise pricing - Volume discounts - Custom deployment options - Academic/nonprofit pricing</p>"},{"location":"about/support/","title":"Support","text":"<p>Get help with Vectorcache.</p>"},{"location":"about/support/#support-channels","title":"Support Channels","text":""},{"location":"about/support/#email-support","title":"\ud83d\udce7 Email Support","text":"<p>support@vectorcache.com</p> <ul> <li>General questions</li> <li>Technical issues</li> <li>Account problems</li> <li>Response time: 24-48 hours</li> </ul>"},{"location":"about/support/#community-discord","title":"\ud83d\udcac Community Discord","text":"<p>Join our Discord</p> <ul> <li>Real-time chat with community</li> <li>Share tips and best practices</li> <li>Get quick answers from other users</li> <li>Connect with the team</li> </ul>"},{"location":"about/support/#documentation","title":"\ud83d\udcda Documentation","text":"<p>docs.vectorcache.ai</p> <ul> <li>Complete API reference</li> <li>Step-by-step guides</li> <li>Code examples</li> <li>Best practices</li> </ul>"},{"location":"about/support/#bug-reports","title":"\ud83d\udc1b Bug Reports","text":"<p>GitHub Issues</p> <ul> <li>Report bugs</li> <li>Request features</li> <li>Track known issues</li> <li>Contribute to docs</li> </ul>"},{"location":"about/support/#support-tiers","title":"Support Tiers","text":""},{"location":"about/support/#free-tier","title":"Free Tier","text":"<ul> <li>Community Discord</li> <li>Documentation</li> <li>GitHub issues</li> <li>Email support (48-hour response)</li> </ul>"},{"location":"about/support/#pro-tier","title":"Pro Tier","text":"<ul> <li>Priority email support (24-hour response)</li> <li>Discord priority channel</li> <li>Video call support (by appointment)</li> </ul>"},{"location":"about/support/#enterprise","title":"Enterprise","text":"<ul> <li>Dedicated support engineer</li> <li>24/7 emergency support</li> <li>Custom SLA</li> <li>On-boarding assistance</li> </ul>"},{"location":"about/support/#common-issues","title":"Common Issues","text":""},{"location":"about/support/#authentication-errors","title":"Authentication Errors","text":"<p>Problem: <code>401 Unauthorized</code></p> <p>Solution: 1. Verify API key in dashboard 2. Check <code>Authorization</code> header format: <code>Bearer YOUR_KEY</code> 3. Ensure key hasn't been revoked</p> <p>See Authentication</p>"},{"location":"about/support/#low-cache-hit-rate","title":"Low Cache Hit Rate","text":"<p>Problem: &lt; 20% cache hit rate</p> <p>Solution: 1. Lower similarity threshold (try 0.80-0.85) 2. Normalize user input 3. Check if queries are actually similar 4. Use context segmentation</p> <p>See Similarity Tuning</p>"},{"location":"about/support/#rate-limit-errors","title":"Rate Limit Errors","text":"<p>Problem: <code>429 Too Many Requests</code></p> <p>Solution: 1. Implement exponential backoff 2. Check rate limit headers 3. Upgrade to higher tier if needed</p> <p>See Error Handling</p>"},{"location":"about/support/#llm-provider-errors","title":"LLM Provider Errors","text":"<p>Problem: <code>502 Bad Gateway - LLM provider error</code></p> <p>Solution: 1. Verify LLM API keys in Settings \u2192 LLM Keys 2. Check LLM provider status 3. Ensure sufficient credits with provider</p>"},{"location":"about/support/#enterprise-support","title":"Enterprise Support","text":"<p>For enterprise customers:</p> <ul> <li>24/7 Support: Phone, email, Slack</li> <li>Dedicated Engineer: Named support contact</li> <li>Custom SLA: 99.9% uptime guarantee</li> <li>Priority Fixes: Bug fixes within 24 hours</li> <li>On-boarding: Dedicated implementation support</li> </ul> <p>Contact sales@vectorcache.com</p>"},{"location":"about/support/#status-page","title":"Status Page","text":"<p>Check service status: status.vectorcache.com</p> <p>Subscribe to updates: - Email notifications - SMS alerts (Enterprise) - Slack integration (Enterprise)</p>"},{"location":"about/support/#office-hours","title":"Office Hours","text":"<p>Join our weekly office hours:</p> <p>Every Wednesday at 2 PM PST - Ask questions live - Demo your use case - Get expert advice - Meet the team</p> <p>Join office hours \u2192</p>"},{"location":"about/support/#feature-requests","title":"Feature Requests","text":"<p>Have an idea? We want to hear it!</p> <ol> <li>Check existing requests: GitHub Discussions</li> <li>Upvote: Vote for features you want</li> <li>Submit new: Create a new discussion</li> </ol> <p>Popular requests often ship within weeks!</p>"},{"location":"about/support/#security-issues","title":"Security Issues","text":"<p>DO NOT report security issues publicly.</p> <p>Email: security@vectorcache.com</p> <p>We'll respond within 24 hours and work with you to: - Understand the issue - Develop a fix - Coordinate disclosure</p>"},{"location":"about/support/#feedback","title":"Feedback","text":"<p>We love feedback!</p> <ul> <li>Product feedback: feedback@vectorcache.com</li> <li>Documentation: Suggest edits via GitHub</li> <li>General thoughts: support@vectorcache.com</li> </ul>"},{"location":"about/support/#contact-sales","title":"Contact Sales","text":"<p>For enterprise, custom solutions, or partnerships:</p> <p>\ud83d\udce7 sales@vectorcache.com</p> <p>We can help with: - Enterprise pricing - Custom deployment - Volume discounts - Integration support - Training and on-boarding</p> <p>Need help right now?</p> <p>Email Support | Join Discord | View Docs</p>"},{"location":"api/authentication/","title":"Authentication","text":"<p>Learn how to authenticate your API requests to Vectorcache.</p>"},{"location":"api/authentication/#api-keys","title":"API Keys","text":"<p>Vectorcache uses API keys for authentication. Each API key is associated with a specific project.</p>"},{"location":"api/authentication/#creating-an-api-key","title":"Creating an API Key","text":"<ol> <li>Log in to your dashboard</li> <li>Navigate to your project</li> <li>Go to the API Keys tab</li> <li>Click Create API Key</li> <li>Give it a descriptive name</li> <li>Copy the key immediately - you won't see it again!</li> </ol>"},{"location":"api/authentication/#api-key-format","title":"API Key Format","text":"<p>API keys follow this format:</p> <pre><code>vc_1234567890abcdef1234567890abcdef\n</code></pre> <ul> <li>Prefix: <code>vc_</code></li> <li>32 hexadecimal characters</li> </ul>"},{"location":"api/authentication/#using-api-keys","title":"Using API Keys","text":""},{"location":"api/authentication/#http-header","title":"HTTP Header","text":"<p>Include your API key in the <code>Authorization</code> header using Bearer authentication:</p> <pre><code>Authorization: Bearer vc_1234567890abcdef1234567890abcdef\n</code></pre>"},{"location":"api/authentication/#example-requests","title":"Example Requests","text":"cURLJavaScriptPython <pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer vc_1234567890abcdef1234567890abcdef\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is AI?\",\n    \"model\": \"gpt-4o\"\n  }'\n</code></pre> <pre><code>const response = await fetch('https://api.vectorcache.ai/v1/cache/query', {\n  method: 'POST',\n  headers: {\n    'Authorization': 'Bearer vc_1234567890abcdef1234567890abcdef',\n    'Content-Type': 'application/json'\n  },\n  body: JSON.stringify({\n    prompt: 'What is AI?',\n    model: 'gpt-4o'\n  })\n});\n</code></pre> <pre><code>import requests\n\nheaders = {\n    'Authorization': 'Bearer vc_1234567890abcdef1234567890abcdef',\n    'Content-Type': 'application/json'\n}\n\nresponse = requests.post(\n    'https://api.vectorcache.ai/v1/cache/query',\n    json={'prompt': 'What is AI?', 'model': 'gpt-4o'},\n    headers=headers\n)\n</code></pre>"},{"location":"api/authentication/#security-best-practices","title":"Security Best Practices","text":""},{"location":"api/authentication/#1-store-keys-securely","title":"1. Store Keys Securely","text":"<p>\u274c Never hardcode API keys:</p> <pre><code>// DON'T DO THIS\nconst apiKey = 'vc_1234567890abcdef1234567890abcdef';\n</code></pre> <p>\u2705 Use environment variables:</p> <pre><code>// DO THIS\nconst apiKey = process.env.VECTORCACHE_API_KEY;\n</code></pre>"},{"location":"api/authentication/#2-use-environment-variables","title":"2. Use Environment Variables","text":"<p>.env file: <pre><code>VECTORCACHE_API_KEY=vc_1234567890abcdef1234567890abcdef\n</code></pre></p> <p>Never commit .env files to git: <pre><code># .gitignore\n.env\n.env.local\n</code></pre></p>"},{"location":"api/authentication/#3-rotate-keys-regularly","title":"3. Rotate Keys Regularly","text":"<ul> <li>Create a new API key</li> <li>Update your application to use the new key</li> <li>Revoke the old key once migration is complete</li> </ul>"},{"location":"api/authentication/#4-use-different-keys-per-environment","title":"4. Use Different Keys per Environment","text":"<pre><code># .env.development\nVECTORCACHE_API_KEY=vc_dev_key123...\n\n# .env.production\nVECTORCACHE_API_KEY=vc_prod_key456...\n</code></pre>"},{"location":"api/authentication/#5-limit-key-exposure","title":"5. Limit Key Exposure","text":"<ul> <li>Use separate keys for different applications</li> <li>Revoke keys when they're no longer needed</li> <li>Monitor key usage in the dashboard</li> </ul>"},{"location":"api/authentication/#managing-api-keys","title":"Managing API Keys","text":""},{"location":"api/authentication/#viewing-api-keys","title":"Viewing API Keys","text":"<ol> <li>Go to your project dashboard</li> <li>Navigate to API Keys tab</li> <li>View all active and revoked keys</li> </ol> <p>You'll see: - Key prefix (e.g., <code>vc_1234...</code>) - Key name - Creation date - Last used date - Status (Active/Revoked)</p>"},{"location":"api/authentication/#revoking-api-keys","title":"Revoking API Keys","text":"<p>To revoke a key:</p> <ol> <li>Go to API Keys tab</li> <li>Find the key to revoke</li> <li>Click Revoke</li> <li>Confirm the action</li> </ol> <p>\u26a0\ufe0f Warning: Revoking a key immediately disables it. Any applications using that key will receive 401 errors.</p>"},{"location":"api/authentication/#key-usage-tracking","title":"Key Usage Tracking","text":"<p>Monitor your API key usage:</p> <ul> <li>Last Used: When the key was last used</li> <li>Request Count: Total requests made with this key</li> <li>Usage Metrics: Available in the Analytics tab</li> </ul>"},{"location":"api/authentication/#authentication-errors","title":"Authentication Errors","text":""},{"location":"api/authentication/#401-unauthorized","title":"401 Unauthorized","text":"<p>Missing or invalid API key:</p> <pre><code>{\n  \"detail\": \"Invalid or missing API key\"\n}\n</code></pre> <p>Causes: - API key not provided - Invalid API key format - Key has been revoked - Key doesn't exist</p> <p>Solution: - Verify your API key is correct - Check if the key is active in your dashboard - Ensure the <code>Authorization</code> header is properly formatted</p>"},{"location":"api/authentication/#403-forbidden","title":"403 Forbidden","text":"<p>API key doesn't have permission:</p> <pre><code>{\n  \"detail\": \"API key does not have permission to access this project\"\n}\n</code></pre> <p>Causes: - Using an API key from a different project - Key permissions have been restricted</p> <p>Solution: - Use the correct API key for this project - Check key permissions in the dashboard</p>"},{"location":"api/authentication/#api-key-scopes-coming-soon","title":"API Key Scopes (Coming Soon)","text":"<p>Future releases will support scoped API keys with granular permissions:</p> <ul> <li><code>cache:read</code> - Read from cache only</li> <li><code>cache:write</code> - Write to cache only</li> <li><code>cache:*</code> - Full cache access</li> <li><code>analytics:read</code> - View analytics</li> <li><code>keys:manage</code> - Manage API keys</li> </ul>"},{"location":"api/authentication/#next-steps","title":"Next Steps","text":"<ul> <li>Cache Endpoints - Use the cache API</li> <li>Error Handling - Handle authentication errors</li> <li>Best Practices - Production security tips</li> </ul>"},{"location":"api/cache/","title":"Cache Endpoints","text":"<p>Complete reference for Vectorcache API endpoints.</p>"},{"location":"api/cache/#query-cache","title":"Query Cache","text":"<p>Query the semantic cache for a response.</p>"},{"location":"api/cache/#endpoint","title":"Endpoint","text":"<pre><code>POST /v1/cache/query\n</code></pre>"},{"location":"api/cache/#request","title":"Request","text":""},{"location":"api/cache/#headers","title":"Headers","text":"Header Value Required <code>Authorization</code> <code>Bearer YOUR_API_KEY</code> Yes <code>Content-Type</code> <code>application/json</code> Yes"},{"location":"api/cache/#body-parameters","title":"Body Parameters","text":"Parameter Type Required Default Description <code>prompt</code> string Yes - The text prompt to cache/query <code>model</code> string Yes - LLM model identifier <code>similarity_threshold</code> number No <code>0.85</code> Minimum similarity score (0-1) <code>context</code> string No <code>null</code> Additional context for segmentation <code>include_debug</code> boolean No <code>false</code> Include debug information"},{"location":"api/cache/#example-request","title":"Example Request","text":"<pre><code>{\n  \"prompt\": \"What is machine learning?\",\n  \"model\": \"gpt-4o\",\n  \"similarity_threshold\": 0.85,\n  \"context\": \"educational-content\",\n  \"include_debug\": false\n}\n</code></pre>"},{"location":"api/cache/#response","title":"Response","text":""},{"location":"api/cache/#success-response-200-ok","title":"Success Response (200 OK)","text":""},{"location":"api/cache/#cache-hit","title":"Cache Hit","text":"<pre><code>{\n  \"cache_hit\": true,\n  \"response\": \"Machine learning is a subset of artificial intelligence...\",\n  \"similarity_score\": 0.92,\n  \"cost_saved\": 0.003,\n  \"llm_provider\": \"cache\"\n}\n</code></pre>"},{"location":"api/cache/#cache-miss","title":"Cache Miss","text":"<pre><code>{\n  \"cache_hit\": false,\n  \"response\": \"Machine learning is a subset of artificial intelligence...\",\n  \"similarity_score\": null,\n  \"cost_saved\": 0,\n  \"llm_provider\": \"openai\"\n}\n</code></pre>"},{"location":"api/cache/#response-fields","title":"Response Fields","text":"Field Type Description <code>cache_hit</code> boolean Whether the query matched a cached entry <code>response</code> string The LLM response text <code>similarity_score</code> number | null Cosine similarity score (0-1), null on cache miss <code>cost_saved</code> number Estimated cost saved in USD (0 on cache miss) <code>llm_provider</code> string Source of response ('cache' or LLM provider name) <code>debug</code> object Debug information (only if <code>include_debug: true</code>)"},{"location":"api/cache/#debug-information","title":"Debug Information","text":"<p>When <code>include_debug: true</code>:</p> <pre><code>{\n  \"cache_hit\": true,\n  \"response\": \"...\",\n  \"similarity_score\": 0.92,\n  \"cost_saved\": 0.003,\n  \"llm_provider\": \"cache\",\n  \"debug\": {\n    \"embedding_time_ms\": 45,\n    \"search_time_ms\": 12,\n    \"total_time_ms\": 57,\n    \"matched_cache_entry_id\": \"uuid-here\",\n    \"cache_entry_count\": 1523\n  }\n}\n</code></pre>"},{"location":"api/cache/#error-responses","title":"Error Responses","text":""},{"location":"api/cache/#400-bad-request","title":"400 Bad Request","text":"<p>Invalid request parameters:</p> <pre><code>{\n  \"detail\": \"similarity_threshold must be between 0 and 1\"\n}\n</code></pre> <p>Common causes: - Missing required fields (<code>prompt</code>, <code>model</code>) - Invalid <code>similarity_threshold</code> (not between 0-1) - Invalid JSON format</p>"},{"location":"api/cache/#401-unauthorized","title":"401 Unauthorized","text":"<p>Authentication failed:</p> <pre><code>{\n  \"detail\": \"Invalid or missing API key\"\n}\n</code></pre> <p>Causes: - Missing <code>Authorization</code> header - Invalid API key - Revoked API key</p>"},{"location":"api/cache/#429-too-many-requests","title":"429 Too Many Requests","text":"<p>Rate limit exceeded:</p> <pre><code>{\n  \"detail\": \"Rate limit exceeded. Please try again later.\"\n}\n</code></pre> <p>Headers included: <pre><code>X-RateLimit-Limit: 100\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1640000000\n</code></pre></p>"},{"location":"api/cache/#500-internal-server-error","title":"500 Internal Server Error","text":"<p>Server error:</p> <pre><code>{\n  \"detail\": \"Internal server error. Please try again or contact support.\"\n}\n</code></pre>"},{"location":"api/cache/#examples","title":"Examples","text":""},{"location":"api/cache/#basic-query","title":"Basic Query","text":"<pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is AI?\",\n    \"model\": \"gpt-4o\"\n  }'\n</code></pre>"},{"location":"api/cache/#with-context","title":"With Context","text":"<pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Explain neural networks\",\n    \"context\": \"technical-documentation\",\n    \"model\": \"gpt-4o\",\n    \"similarity_threshold\": 0.90\n  }'\n</code></pre>"},{"location":"api/cache/#with-debug-info","title":"With Debug Info","text":"<pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is deep learning?\",\n    \"model\": \"gpt-4o\",\n    \"include_debug\": true\n  }'\n</code></pre>"},{"location":"api/cache/#supported-models","title":"Supported Models","text":"<p>Vectorcache supports all major LLM providers:</p>"},{"location":"api/cache/#openai","title":"OpenAI","text":"<ul> <li><code>gpt-4o</code></li> <li><code>gpt-4o-mini</code></li> <li><code>gpt-4-turbo</code></li> <li><code>gpt-3.5-turbo</code></li> </ul>"},{"location":"api/cache/#anthropic","title":"Anthropic","text":"<ul> <li><code>claude-3-5-sonnet-20241022</code></li> <li><code>claude-3-5-haiku-20241022</code></li> <li><code>claude-3-opus-20240229</code></li> </ul>"},{"location":"api/cache/#google","title":"Google","text":"<ul> <li><code>gemini-1.5-pro</code></li> <li><code>gemini-1.5-flash</code></li> </ul>"},{"location":"api/cache/#other-providers","title":"Other Providers","text":"<p>Check the dashboard for your configured LLM providers.</p>"},{"location":"api/cache/#context-based-segmentation","title":"Context-Based Segmentation","text":"<p>Use the <code>context</code> parameter to segment your cache by use case:</p> <pre><code>// Educational content cache\nawait client.query({\n  prompt: 'What is photosynthesis?',\n  context: 'education-biology',\n  model: 'gpt-4o'\n});\n\n// Technical documentation cache\nawait client.query({\n  prompt: 'What is photosynthesis?',\n  context: 'scientific-research',\n  model: 'gpt-4o'\n});\n</code></pre> <p>Even with the same prompt, these will be cached separately due to different contexts.</p>"},{"location":"api/cache/#similarity-threshold","title":"Similarity Threshold","text":"<p>The <code>similarity_threshold</code> parameter controls cache sensitivity:</p> Threshold Behavior Use Case 0.95-1.0 Very strict Legal, medical, financial content 0.85-0.94 Recommended General purpose, customer support 0.75-0.84 Relaxed Educational content, FAQs &lt;0.75 Very relaxed Not recommended (low accuracy)"},{"location":"api/cache/#example-testing-thresholds","title":"Example: Testing Thresholds","text":"<pre><code>// Strict - only nearly identical queries match\nconst strict = await client.query({\n  prompt: 'What is machine learning?',\n  model: 'gpt-4o',\n  similarityThreshold: 0.95\n});\n\n// Relaxed - more cache hits, less precision\nconst relaxed = await client.query({\n  prompt: 'What is machine learning?',\n  model: 'gpt-4o',\n  similarityThreshold: 0.80\n});\n</code></pre>"},{"location":"api/cache/#cost-calculation","title":"Cost Calculation","text":"<p>The <code>cost_saved</code> field estimates the LLM API cost you saved from the cache hit:</p> <pre><code>{\n  \"cache_hit\": true,\n  \"cost_saved\": 0.003,  // $0.003 saved\n  \"llm_provider\": \"cache\"\n}\n</code></pre> <p>Calculation: - Based on the model's input/output token pricing - Includes both prompt and response tokens - Updated automatically with latest pricing</p> <p>Example savings: - <code>gpt-4o</code> query: ~$0.002-0.005 per query saved - 1,000 cache hits/day: ~$2-5 saved/day - Annual savings: ~$730-1,825</p>"},{"location":"api/cache/#performance","title":"Performance","text":""},{"location":"api/cache/#response-times","title":"Response Times","text":"Scenario Typical Response Time Cache Hit 50-150ms Cache Miss (with LLM call) 1-5 seconds Debug Mode +10-20ms"},{"location":"api/cache/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use appropriate thresholds - Higher thresholds = faster searches</li> <li>Enable caching - First query is slow, subsequent ones are fast</li> <li>Batch similar queries - Group related prompts together</li> <li>Monitor debug metrics - Use <code>include_debug</code> to optimize</li> </ol>"},{"location":"api/cache/#rate-limits","title":"Rate Limits","text":"Tier Requests/Minute Burst Free 100 120 Pro 1,000 1,200 Enterprise Custom Custom <p>Rate limit headers: <pre><code>X-RateLimit-Limit: 100\nX-RateLimit-Remaining: 95\nX-RateLimit-Reset: 1640000000\n</code></pre></p>"},{"location":"api/cache/#best-practices","title":"Best Practices","text":"<ol> <li>Always handle both cache hit and miss - Your application should work in both scenarios</li> <li>Use context for segmentation - Separate caches by use case</li> <li>Monitor similarity scores - Tune thresholds based on actual scores</li> <li>Implement retry logic - Handle 429 errors with exponential backoff</li> <li>Track cost savings - Monitor <code>cost_saved</code> to measure ROI</li> </ol>"},{"location":"api/cache/#next-steps","title":"Next Steps","text":"<ul> <li>Error Handling - Handle API errors</li> <li>Best Practices - Production tips</li> <li>Similarity Tuning - Optimize cache hits</li> </ul>"},{"location":"api/errors/","title":"Error Handling","text":"<p>Complete guide to handling errors in the Vectorcache API.</p>"},{"location":"api/errors/#error-response-format","title":"Error Response Format","text":"<p>All errors return JSON with a <code>detail</code> field:</p> <pre><code>{\n  \"detail\": \"Error message describing what went wrong\"\n}\n</code></pre>"},{"location":"api/errors/#http-status-codes","title":"HTTP Status Codes","text":"Code Status Description 200 OK Request successful 400 Bad Request Invalid request parameters 401 Unauthorized Authentication failed 403 Forbidden Insufficient permissions 404 Not Found Resource not found 429 Too Many Requests Rate limit exceeded 500 Internal Server Error Server error 502 Bad Gateway LLM provider error 503 Service Unavailable Service temporarily unavailable"},{"location":"api/errors/#common-errors","title":"Common Errors","text":""},{"location":"api/errors/#400-bad-request","title":"400 Bad Request","text":""},{"location":"api/errors/#missing-required-fields","title":"Missing Required Fields","text":"<pre><code>{\n  \"detail\": \"Field required: prompt\"\n}\n</code></pre> <p>Cause: Missing <code>prompt</code> or <code>model</code> in request</p> <p>Solution: <pre><code>// \u274c Missing model\n{ prompt: \"What is AI?\" }\n\n// \u2705 Include all required fields\n{ prompt: \"What is AI?\", model: \"gpt-4o\" }\n</code></pre></p>"},{"location":"api/errors/#invalid-similarity-threshold","title":"Invalid Similarity Threshold","text":"<pre><code>{\n  \"detail\": \"similarity_threshold must be between 0 and 1\"\n}\n</code></pre> <p>Cause: <code>similarity_threshold</code> outside valid range</p> <p>Solution: <pre><code>// \u274c Invalid threshold\n{ similarity_threshold: 1.5 }\n\n// \u2705 Valid threshold\n{ similarity_threshold: 0.85 }\n</code></pre></p>"},{"location":"api/errors/#invalid-json","title":"Invalid JSON","text":"<pre><code>{\n  \"detail\": \"Invalid JSON format\"\n}\n</code></pre> <p>Cause: Malformed JSON in request body</p> <p>Solution: Validate JSON before sending</p>"},{"location":"api/errors/#401-unauthorized","title":"401 Unauthorized","text":""},{"location":"api/errors/#missing-api-key","title":"Missing API Key","text":"<pre><code>{\n  \"detail\": \"Invalid or missing API key\"\n}\n</code></pre> <p>Cause: No <code>Authorization</code> header</p> <p>Solution: <pre><code>// \u274c Missing auth header\nfetch(url, {\n  method: 'POST',\n  body: JSON.stringify(data)\n});\n\n// \u2705 Include auth header\nfetch(url, {\n  method: 'POST',\n  headers: {\n    'Authorization': `Bearer ${apiKey}`\n  },\n  body: JSON.stringify(data)\n});\n</code></pre></p>"},{"location":"api/errors/#invalid-api-key","title":"Invalid API Key","text":"<pre><code>{\n  \"detail\": \"Invalid API key\"\n}\n</code></pre> <p>Cause: API key is invalid or revoked</p> <p>Solution: - Verify your API key in the dashboard - Check if the key has been revoked - Create a new API key if needed</p>"},{"location":"api/errors/#403-forbidden","title":"403 Forbidden","text":""},{"location":"api/errors/#insufficient-permissions","title":"Insufficient Permissions","text":"<pre><code>{\n  \"detail\": \"API key does not have permission to access this project\"\n}\n</code></pre> <p>Cause: Using an API key from a different project</p> <p>Solution: Use the correct API key for this project</p>"},{"location":"api/errors/#429-too-many-requests","title":"429 Too Many Requests","text":""},{"location":"api/errors/#rate-limit-exceeded","title":"Rate Limit Exceeded","text":"<pre><code>{\n  \"detail\": \"Rate limit exceeded. Please try again later.\"\n}\n</code></pre> <p>Headers: <pre><code>X-RateLimit-Limit: 100\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset: 1640000000\n</code></pre></p> <p>Solution: Implement retry with exponential backoff</p> <pre><code>async function queryWithRetry(client, request, maxRetries = 3) {\n  for (let attempt = 1; attempt &lt;= maxRetries; attempt++) {\n    try {\n      return await client.query(request);\n    } catch (error) {\n      if (error.statusCode === 429) {\n        const resetTime = error.headers['x-ratelimit-reset'];\n        const waitTime = Math.max(\n          (resetTime * 1000) - Date.now(),\n          1000 * Math.pow(2, attempt)\n        );\n        await new Promise(resolve =&gt; setTimeout(resolve, waitTime));\n        continue;\n      }\n      throw error;\n    }\n  }\n}\n</code></pre>"},{"location":"api/errors/#500-internal-server-error","title":"500 Internal Server Error","text":""},{"location":"api/errors/#server-error","title":"Server Error","text":"<pre><code>{\n  \"detail\": \"Internal server error. Please try again or contact support.\"\n}\n</code></pre> <p>Cause: Unexpected server error</p> <p>Solution: - Retry the request - If persists, contact support with request details</p>"},{"location":"api/errors/#502-bad-gateway","title":"502 Bad Gateway","text":""},{"location":"api/errors/#llm-provider-error","title":"LLM Provider Error","text":"<pre><code>{\n  \"detail\": \"LLM provider error: Invalid API key\"\n}\n</code></pre> <p>Cause: Issue with your LLM provider API key</p> <p>Solution: - Verify your LLM API key in Settings \u2192 LLM Keys - Check if you have sufficient credits with the provider - Ensure the model is available for your LLM provider</p>"},{"location":"api/errors/#503-service-unavailable","title":"503 Service Unavailable","text":""},{"location":"api/errors/#service-temporarily-unavailable","title":"Service Temporarily Unavailable","text":"<pre><code>{\n  \"detail\": \"Service temporarily unavailable\"\n}\n</code></pre> <p>Cause: Service maintenance or high load</p> <p>Solution: Retry with exponential backoff</p>"},{"location":"api/errors/#error-handling-patterns","title":"Error Handling Patterns","text":""},{"location":"api/errors/#javascripttypescript","title":"JavaScript/TypeScript","text":"<pre><code>import { VectorcacheClient, VectorcacheError } from 'vectorcache';\n\nconst client = new VectorcacheClient({ apiKey: 'YOUR_API_KEY' });\n\ntry {\n  const result = await client.query({\n    prompt: 'What is AI?',\n    model: 'gpt-4o'\n  });\n\n  console.log(result.response);\n\n} catch (error) {\n  if (error instanceof VectorcacheError) {\n    switch (error.statusCode) {\n      case 400:\n        console.error('Invalid request:', error.message);\n        break;\n      case 401:\n        console.error('Authentication failed - check your API key');\n        break;\n      case 429:\n        console.error('Rate limit exceeded - please slow down');\n        break;\n      case 500:\n        console.error('Server error - retrying...');\n        // Implement retry logic\n        break;\n      case 502:\n        console.error('LLM provider error:', error.message);\n        break;\n      default:\n        console.error('Unexpected error:', error.message);\n    }\n  } else {\n    console.error('Network or unknown error:', error);\n  }\n}\n</code></pre>"},{"location":"api/errors/#python","title":"Python","text":"<pre><code>import requests\nfrom requests.exceptions import HTTPError, Timeout, RequestException\n\ndef query_with_error_handling(prompt: str, model: str, api_key: str):\n    url = 'https://api.vectorcache.ai/v1/cache/query'\n\n    headers = {\n        'Authorization': f'Bearer {api_key}',\n        'Content-Type': 'application/json'\n    }\n\n    data = {\n        'prompt': prompt,\n        'model': model\n    }\n\n    try:\n        response = requests.post(url, json=data, headers=headers, timeout=30)\n        response.raise_for_status()\n        return response.json()\n\n    except HTTPError as e:\n        status_code = e.response.status_code\n        error_detail = e.response.json().get('detail', 'Unknown error')\n\n        if status_code == 400:\n            raise ValueError(f\"Invalid request: {error_detail}\")\n        elif status_code == 401:\n            raise ValueError(\"Authentication failed - check your API key\")\n        elif status_code == 429:\n            raise ValueError(\"Rate limit exceeded - please slow down\")\n        elif status_code == 500:\n            raise ValueError(f\"Server error: {error_detail}\")\n        elif status_code == 502:\n            raise ValueError(f\"LLM provider error: {error_detail}\")\n        else:\n            raise ValueError(f\"HTTP error {status_code}: {error_detail}\")\n\n    except Timeout:\n        raise ValueError(\"Request timed out\")\n\n    except RequestException as e:\n        raise ValueError(f\"Request failed: {e}\")\n</code></pre>"},{"location":"api/errors/#retry-strategies","title":"Retry Strategies","text":""},{"location":"api/errors/#exponential-backoff","title":"Exponential Backoff","text":"<pre><code>async function exponentialBackoff(\n  fn,\n  maxRetries = 3,\n  baseDelay = 1000\n) {\n  for (let attempt = 1; attempt &lt;= maxRetries; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      // Don't retry on client errors (4xx except 429)\n      if (error.statusCode &gt;= 400 &amp;&amp;\n          error.statusCode &lt; 500 &amp;&amp;\n          error.statusCode !== 429) {\n        throw error;\n      }\n\n      if (attempt === maxRetries) {\n        throw error;\n      }\n\n      const delay = Math.min(baseDelay * Math.pow(2, attempt - 1), 10000);\n      console.log(`Retry attempt ${attempt} after ${delay}ms`);\n      await new Promise(resolve =&gt; setTimeout(resolve, delay));\n    }\n  }\n}\n\n// Usage\nconst result = await exponentialBackoff(\n  () =&gt; client.query({ prompt: 'What is AI?', model: 'gpt-4o' })\n);\n</code></pre>"},{"location":"api/errors/#rate-limit-aware-retry","title":"Rate Limit Aware Retry","text":"<pre><code>import time\nfrom datetime import datetime\n\ndef retry_with_rate_limit(fn, max_retries=3):\n    for attempt in range(1, max_retries + 1):\n        try:\n            return fn()\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:\n                # Check rate limit reset time\n                reset_time = int(e.response.headers.get('X-RateLimit-Reset', 0))\n                if reset_time:\n                    wait_time = max(reset_time - int(time.time()), 0) + 1\n                else:\n                    wait_time = min(2 ** attempt, 60)\n\n                print(f\"Rate limited. Waiting {wait_time}s...\")\n                time.sleep(wait_time)\n                continue\n            raise\n        except Exception as e:\n            if attempt == max_retries:\n                raise\n            delay = min(2 ** attempt, 10)\n            print(f\"Retry {attempt} after {delay}s\")\n            time.sleep(delay)\n</code></pre>"},{"location":"api/errors/#validation-best-practices","title":"Validation Best Practices","text":""},{"location":"api/errors/#client-side-validation","title":"Client-Side Validation","text":"<p>Validate inputs before making API calls:</p> <pre><code>function validateQueryRequest(request: CacheQueryRequest): void {\n  if (!request.prompt || request.prompt.trim().length === 0) {\n    throw new Error('Prompt is required');\n  }\n\n  if (!request.model || request.model.trim().length === 0) {\n    throw new Error('Model is required');\n  }\n\n  if (request.similarityThreshold !== undefined) {\n    if (request.similarityThreshold &lt; 0 || request.similarityThreshold &gt; 1) {\n      throw new Error('Similarity threshold must be between 0 and 1');\n    }\n  }\n}\n\n// Usage\ntry {\n  validateQueryRequest(request);\n  const result = await client.query(request);\n} catch (error) {\n  console.error('Validation error:', error.message);\n}\n</code></pre>"},{"location":"api/errors/#graceful-degradation","title":"Graceful Degradation","text":"<p>Implement fallback logic when Vectorcache is unavailable:</p> <pre><code>async function queryWithFallback(prompt: string, model: string) {\n  try {\n    // Try Vectorcache first\n    const result = await client.query({ prompt, model });\n    return result.response;\n  } catch (error) {\n    console.warn('Vectorcache unavailable, falling back to direct LLM');\n    // Fallback to direct LLM call\n    return await directLLMCall(prompt, model);\n  }\n}\n</code></pre>"},{"location":"api/errors/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"api/errors/#log-error-details","title":"Log Error Details","text":"<pre><code>function logError(error: VectorcacheError, context: any) {\n  const errorLog = {\n    timestamp: new Date().toISOString(),\n    statusCode: error.statusCode,\n    message: error.message,\n    context: context,\n    headers: error.headers\n  };\n\n  // Send to logging service\n  logger.error('Vectorcache API error', errorLog);\n\n  // For 5xx errors, alert operations team\n  if (error.statusCode &gt;= 500) {\n    alertOps(errorLog);\n  }\n}\n</code></pre>"},{"location":"api/errors/#track-error-rates","title":"Track Error Rates","text":"<pre><code>const errorMetrics = {\n  total: 0,\n  byStatusCode: {} as Record&lt;number, number&gt;\n};\n\nfunction trackError(error: VectorcacheError) {\n  errorMetrics.total++;\n  errorMetrics.byStatusCode[error.statusCode] =\n    (errorMetrics.byStatusCode[error.statusCode] || 0) + 1;\n\n  // Alert if error rate is high\n  const errorRate = errorMetrics.total / totalRequests;\n  if (errorRate &gt; 0.05) { // 5% error rate\n    alertHighErrorRate(errorMetrics);\n  }\n}\n</code></pre>"},{"location":"api/errors/#next-steps","title":"Next Steps","text":"<ul> <li>Best Practices - Production deployment tips</li> <li>API Reference - Complete API documentation</li> <li>Support - Get help with errors</li> </ul>"},{"location":"api/overview/","title":"API Overview","text":"<p>Vectorcache provides a RESTful API for semantic caching of LLM responses.</p>"},{"location":"api/overview/#base-url","title":"Base URL","text":"<pre><code>https://api.vectorcache.ai\n</code></pre>"},{"location":"api/overview/#authentication","title":"Authentication","text":"<p>All API requests require authentication using an API key in the Authorization header:</p> <pre><code>Authorization: Bearer YOUR_API_KEY\n</code></pre> <p>Get your API key from the dashboard.</p>"},{"location":"api/overview/#endpoints","title":"Endpoints","text":"Method Endpoint Description POST <code>/v1/cache/query</code> Query the semantic cache"},{"location":"api/overview/#request-format","title":"Request Format","text":"<p>All requests must include:</p> <ul> <li><code>Content-Type: application/json</code> header</li> <li>JSON request body with required parameters</li> <li>Bearer token authentication</li> </ul>"},{"location":"api/overview/#response-format","title":"Response Format","text":"<p>All successful responses return JSON with:</p> <pre><code>{\n  \"cache_hit\": boolean,\n  \"response\": string,\n  \"similarity_score\": number | null,\n  \"cost_saved\": number,\n  \"llm_provider\": string\n}\n</code></pre>"},{"location":"api/overview/#rate-limits","title":"Rate Limits","text":"<ul> <li>Free tier: 100 requests/minute</li> <li>Pro tier: 1,000 requests/minute</li> <li>Enterprise: Custom limits</li> </ul> <p>Rate limit headers are included in responses:</p> <pre><code>X-RateLimit-Limit: 100\nX-RateLimit-Remaining: 95\nX-RateLimit-Reset: 1640000000\n</code></pre>"},{"location":"api/overview/#versioning","title":"Versioning","text":"<p>The API is versioned via the URL path (<code>/v1/</code>). Breaking changes will result in a new version.</p> <p>Current version: v1</p>"},{"location":"api/overview/#error-handling","title":"Error Handling","text":"<p>All errors return a JSON response with a <code>detail</code> field:</p> <pre><code>{\n  \"detail\": \"Error message describing what went wrong\"\n}\n</code></pre> <p>See Error Handling for complete error codes and handling.</p>"},{"location":"api/overview/#quick-example","title":"Quick Example","text":"<pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is machine learning?\",\n    \"model\": \"gpt-4o\",\n    \"similarity_threshold\": 0.85\n  }'\n</code></pre>"},{"location":"api/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Authentication - API key management</li> <li>Cache Endpoints - Detailed endpoint documentation</li> <li>Error Handling - Error codes and handling</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Configure the Vectorcache SDK for your application.</p>"},{"location":"getting-started/configuration/#javascripttypescript-configuration","title":"JavaScript/TypeScript Configuration","text":""},{"location":"getting-started/configuration/#basic-setup","title":"Basic Setup","text":"<pre><code>import { VectorcacheClient } from 'vectorcache';\n\nconst client = new VectorcacheClient({\n  apiKey: 'your_api_key_here',\n  baseUrl: 'https://api.vectorcache.ai' // optional, defaults to production\n});\n</code></pre>"},{"location":"getting-started/configuration/#configuration-options","title":"Configuration Options","text":"Option Type Required Default Description <code>apiKey</code> string Yes - Your Vectorcache API key <code>baseUrl</code> string No <code>https://api.vectorcache.ai</code> API base URL <code>timeout</code> number No <code>30000</code> Request timeout in milliseconds"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Store your API key in environment variables:</p> <pre><code>const client = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!,\n});\n</code></pre> <p><code>.env</code> file: <pre><code>VECTORCACHE_API_KEY=your_api_key_here\n</code></pre></p>"},{"location":"getting-started/configuration/#typescript-types","title":"TypeScript Types","text":"<p>The SDK is fully typed. Import types as needed:</p> <pre><code>import {\n  VectorcacheClient,\n  CacheQueryRequest,\n  CacheQueryResponse\n} from 'vectorcache';\n\nconst request: CacheQueryRequest = {\n  prompt: 'What is AI?',\n  model: 'gpt-4o',\n  similarityThreshold: 0.85\n};\n\nconst response: CacheQueryResponse = await client.query(request);\n</code></pre>"},{"location":"getting-started/configuration/#python-configuration","title":"Python Configuration","text":""},{"location":"getting-started/configuration/#basic-setup_1","title":"Basic Setup","text":"<pre><code>import requests\nimport os\n\napi_key = os.environ.get('VECTORCACHE_API_KEY')\nbase_url = 'https://api.vectorcache.ai'\n\nheaders = {\n    'Authorization': f'Bearer {api_key}',\n    'Content-Type': 'application/json'\n}\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables_1","title":"Environment Variables","text":"<p><code>.env</code> file: <pre><code>VECTORCACHE_API_KEY=your_api_key_here\n</code></pre></p> <p>Using python-dotenv: <pre><code>from dotenv import load_dotenv\nimport os\n\nload_dotenv()\n\napi_key = os.environ.get('VECTORCACHE_API_KEY')\n</code></pre></p>"},{"location":"getting-started/configuration/#query-parameters","title":"Query Parameters","text":""},{"location":"getting-started/configuration/#required-parameters","title":"Required Parameters","text":"Parameter Type Description <code>prompt</code> string The text prompt to cache/query <code>model</code> string LLM model identifier (e.g., 'gpt-4o', 'claude-3-5-sonnet-20241022')"},{"location":"getting-started/configuration/#optional-parameters","title":"Optional Parameters","text":"Parameter Type Default Description <code>similarity_threshold</code> number <code>0.85</code> Minimum similarity score (0-1) for cache hits <code>context</code> string <code>null</code> Additional context for the query <code>project_id</code> string Auto-detected Project ID (inferred from API key) <code>include_debug</code> boolean <code>false</code> Include debug information in response"},{"location":"getting-started/configuration/#example-with-all-parameters","title":"Example with All Parameters","text":"<pre><code>const result = await client.query({\n  prompt: 'Explain machine learning',\n  context: 'Educational content for beginners',\n  model: 'gpt-4o',\n  similarityThreshold: 0.85,\n  includeDebug: true\n});\n</code></pre>"},{"location":"getting-started/configuration/#similarity-threshold","title":"Similarity Threshold","text":"<p>The <code>similarity_threshold</code> parameter controls cache sensitivity:</p> <ul> <li>0.95-1.0: Very strict - only nearly identical queries match</li> <li>0.85-0.94: Recommended - good balance of accuracy and hit rate</li> <li>0.70-0.84: Relaxed - more cache hits but less precise matches</li> <li>Below 0.70: Not recommended - may return irrelevant cached responses</li> </ul>"},{"location":"getting-started/configuration/#finding-the-right-threshold","title":"Finding the Right Threshold","text":"<p>Start with <code>0.85</code> and adjust based on your use case:</p> <pre><code>// Educational content - can be more relaxed\nconst eduResult = await client.query({\n  prompt: 'What is photosynthesis?',\n  similarityThreshold: 0.80 // Lower threshold OK\n});\n\n// Legal/Medical - needs precision\nconst legalResult = await client.query({\n  prompt: 'Interpret contract clause 5.2',\n  similarityThreshold: 0.92 // Higher threshold for accuracy\n});\n</code></pre> <p>Learn more in the Similarity Tuning Guide.</p>"},{"location":"getting-started/configuration/#error-handling","title":"Error Handling","text":""},{"location":"getting-started/configuration/#javascripttypescript","title":"JavaScript/TypeScript","text":"<pre><code>try {\n  const result = await client.query({\n    prompt: 'What is AI?',\n    model: 'gpt-4o'\n  });\n  console.log(result.response);\n} catch (error) {\n  if (error.response?.status === 401) {\n    console.error('Invalid API key');\n  } else if (error.response?.status === 429) {\n    console.error('Rate limit exceeded');\n  } else {\n    console.error('Unexpected error:', error.message);\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#python","title":"Python","text":"<pre><code>try:\n    response = requests.post(\n        f\"{base_url}/v1/cache/query\",\n        json=data,\n        headers=headers\n    )\n    response.raise_for_status()\n    result = response.json()\nexcept requests.exceptions.HTTPError as e:\n    if e.response.status_code == 401:\n        print(\"Invalid API key\")\n    elif e.response.status_code == 429:\n        print(\"Rate limit exceeded\")\n    else:\n        print(f\"HTTP error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre> <p>See Error Handling for complete error codes.</p>"},{"location":"getting-started/configuration/#production-best-practices","title":"Production Best Practices","text":""},{"location":"getting-started/configuration/#1-use-environment-variables","title":"1. Use Environment Variables","text":"<p>Never hardcode API keys:</p> <pre><code>// \u274c Bad\nconst client = new VectorcacheClient({\n  apiKey: 'vc_1234567890abcdef'\n});\n\n// \u2705 Good\nconst client = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!\n});\n</code></pre>"},{"location":"getting-started/configuration/#2-set-appropriate-timeouts","title":"2. Set Appropriate Timeouts","text":"<pre><code>const client = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!,\n  timeout: 10000 // 10 seconds for production\n});\n</code></pre>"},{"location":"getting-started/configuration/#3-handle-errors-gracefully","title":"3. Handle Errors Gracefully","text":"<p>Always have fallback logic:</p> <pre><code>async function getResponse(prompt: string) {\n  try {\n    const result = await client.query({ prompt, model: 'gpt-4o' });\n    return result.response;\n  } catch (error) {\n    console.error('Vectorcache error:', error);\n    // Fallback to direct LLM call\n    return await fallbackLLMCall(prompt);\n  }\n}\n</code></pre>"},{"location":"getting-started/configuration/#4-monitor-performance","title":"4. Monitor Performance","text":"<p>Track cache performance in your application:</p> <pre><code>const result = await client.query({ prompt, model: 'gpt-4o' });\n\n// Log metrics\nanalytics.track('vectorcache_query', {\n  cache_hit: result.cache_hit,\n  similarity_score: result.similarity_score,\n  cost_saved: result.cost_saved\n});\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation</li> <li>Best Practices - Production tips</li> <li>Similarity Tuning - Optimize cache hit rates</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Install the Vectorcache SDK for your preferred language.</p>"},{"location":"getting-started/installation/#javascripttypescript","title":"JavaScript/TypeScript","text":""},{"location":"getting-started/installation/#npm","title":"NPM","text":"<pre><code>npm install vectorcache\n</code></pre>"},{"location":"getting-started/installation/#yarn","title":"Yarn","text":"<pre><code>yarn add vectorcache\n</code></pre>"},{"location":"getting-started/installation/#pnpm","title":"pnpm","text":"<pre><code>pnpm add vectorcache\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Node.js 16.x or higher</li> <li>TypeScript 4.5+ (if using TypeScript)</li> </ul>"},{"location":"getting-started/installation/#python","title":"Python","text":""},{"location":"getting-started/installation/#pip","title":"pip","text":"<pre><code>pip install vectorcache-python\n</code></pre>"},{"location":"getting-started/installation/#poetry","title":"Poetry","text":"<pre><code>poetry add vectorcache-python\n</code></pre>"},{"location":"getting-started/installation/#requirements_1","title":"Requirements","text":"<ul> <li>Python 3.8 or higher</li> </ul>"},{"location":"getting-started/installation/#direct-api-no-sdk","title":"Direct API (No SDK)","text":"<p>You can also use Vectorcache without an SDK by making direct HTTP requests to our REST API.</p>"},{"location":"getting-started/installation/#curl","title":"cURL","text":"<p>No installation needed - just use cURL:</p> <pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Your prompt here\",\n    \"model\": \"gpt-4o\",\n    \"similarity_threshold\": 0.85\n  }'\n</code></pre>"},{"location":"getting-started/installation/#http-clients","title":"HTTP Clients","text":"<p>Any HTTP client library works:</p> JavaScript (Axios)Python (httpx) <pre><code>const axios = require('axios');\n\nconst response = await axios.post(\n  'https://api.vectorcache.ai/v1/cache/query',\n  {\n    prompt: 'Your prompt here',\n    model: 'gpt-4o',\n    similarity_threshold: 0.85\n  },\n  {\n    headers: {\n      'Authorization': `Bearer ${apiKey}`,\n      'Content-Type': 'application/json'\n    }\n  }\n);\n</code></pre> <pre><code>import httpx\n\nasync with httpx.AsyncClient() as client:\n    response = await client.post(\n        'https://api.vectorcache.ai/v1/cache/query',\n        json={\n            'prompt': 'Your prompt here',\n            'model': 'gpt-4o',\n            'similarity_threshold': 0.85\n        },\n        headers={\n            'Authorization': f'Bearer {api_key}',\n            'Content-Type': 'application/json'\n        }\n    )\n    result = response.json()\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"JavaScript/TypeScriptPython <pre><code>const { VectorcacheClient } = require('vectorcache');\nconsole.log('Vectorcache SDK installed successfully!');\n</code></pre> <pre><code>import vectorcache\nprint('Vectorcache SDK installed successfully!')\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Make your first request</li> <li>Configuration - Configure the SDK</li> <li>API Reference - Explore all endpoints</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>Get started with Vectorcache in under 5 minutes.</p>"},{"location":"getting-started/quickstart/#step-1-create-an-account","title":"Step 1: Create an Account","text":"<ol> <li>Visit https://app.vectorcache.com</li> <li>Sign up with your email or GitHub account</li> <li>Verify your email address</li> </ol>"},{"location":"getting-started/quickstart/#step-2-create-a-project","title":"Step 2: Create a Project","text":"<ol> <li>Log in to your dashboard</li> <li>Click \"Create New Project\"</li> <li>Give your project a name (e.g., \"My Chatbot\")</li> <li>Click \"Create Project\"</li> </ol>"},{"location":"getting-started/quickstart/#step-3-add-your-llm-api-keys","title":"Step 3: Add Your LLM API Keys","text":"<p>Before caching can work, Vectorcache needs your LLM provider API keys:</p> <ol> <li>Navigate to Settings \u2192 LLM Keys</li> <li>Click \"Add LLM Key\"</li> <li>Select your provider (OpenAI, Anthropic, etc.)</li> <li>Enter your API key</li> <li>Give it a name for reference</li> </ol> <p>Security</p> <p>Your LLM API keys are encrypted at rest and never exposed in logs or responses.</p>"},{"location":"getting-started/quickstart/#step-4-get-your-vectorcache-api-key","title":"Step 4: Get Your Vectorcache API Key","text":"<ol> <li>Go to your project dashboard</li> <li>Navigate to the API Keys tab</li> <li>Click \"Create API Key\"</li> <li>Give it a name (e.g., \"Production Key\")</li> <li>Copy the API key immediately - you won't see it again!</li> </ol>"},{"location":"getting-started/quickstart/#step-5-install-the-sdk","title":"Step 5: Install the SDK","text":"JavaScript/TypeScriptPython <pre><code>npm install vectorcache\n</code></pre> <pre><code>pip install vectorcache-python\n</code></pre>"},{"location":"getting-started/quickstart/#step-6-make-your-first-request","title":"Step 6: Make Your First Request","text":"JavaScript/TypeScriptPython <pre><code>import { VectorcacheClient } from 'vectorcache';\n\nconst client = new VectorcacheClient({\n  apiKey: 'your_api_key_here',\n  baseUrl: 'https://api.vectorcache.ai'\n});\n\nasync function main() {\n  const result = await client.query({\n    prompt: 'Explain quantum computing in simple terms',\n    context: 'Educational content for beginners',\n    model: 'gpt-4o',\n    similarityThreshold: 0.85\n  });\n\n  console.log(`Cache hit: ${result.cache_hit}`);\n  console.log(`Response: ${result.response}`);\n\n  if (result.cost_saved) {\n    console.log(`Cost saved: $${result.cost_saved}`);\n  }\n}\n\nmain();\n</code></pre> <pre><code>import requests\n\napi_key = \"your_api_key_here\"\nbase_url = \"https://api.vectorcache.ai\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {api_key}\",\n    \"Content-Type\": \"application/json\"\n}\n\ndata = {\n    \"prompt\": \"Explain quantum computing in simple terms\",\n    \"context\": \"Educational content for beginners\",\n    \"model\": \"gpt-4o\",\n    \"similarity_threshold\": 0.85\n}\n\nresponse = requests.post(\n    f\"{base_url}/v1/cache/query\",\n    json=data,\n    headers=headers\n)\n\nresult = response.json()\n\nprint(f\"Cache hit: {result['cache_hit']}\")\nprint(f\"Response: {result['response']}\")\n\nif result.get('cost_saved'):\n    print(f\"Cost saved: ${result['cost_saved']}\")\n</code></pre>"},{"location":"getting-started/quickstart/#understanding-the-response","title":"Understanding the Response","text":"<p>Your first request will be a cache miss since nothing is cached yet:</p> <pre><code>{\n  \"cache_hit\": false,\n  \"response\": \"Quantum computing is...\",\n  \"similarity_score\": null,\n  \"cost_saved\": 0,\n  \"llm_provider\": \"openai\"\n}\n</code></pre> <p>The second request with a similar prompt will be a cache hit:</p> <pre><code>{\n  \"cache_hit\": true,\n  \"response\": \"Quantum computing is...\",\n  \"similarity_score\": 0.92,\n  \"cost_saved\": 0.003,\n  \"llm_provider\": \"cache\"\n}\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>\u2705 You're now caching LLM responses!</p> <p>Here's what to explore next:</p> <ul> <li>Tune Similarity Threshold - Optimize cache hit rates</li> <li>View Analytics - Track savings in your dashboard</li> <li>API Reference - Explore all available endpoints</li> <li>Best Practices - Production deployment tips</li> </ul>"},{"location":"getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quickstart/#cache-always-missing","title":"Cache Always Missing","text":"<ul> <li>Ensure <code>similarity_threshold</code> is not too high (try 0.80-0.85)</li> <li>Verify your prompts are actually similar</li> <li>Check that you're using the same <code>model</code> parameter</li> </ul>"},{"location":"getting-started/quickstart/#authentication-errors","title":"Authentication Errors","text":"<ul> <li>Confirm your API key is correctly formatted</li> <li>Check that the key hasn't been revoked</li> <li>Ensure you're using the correct project API key</li> </ul>"},{"location":"getting-started/quickstart/#llm-api-errors","title":"LLM API Errors","text":"<ul> <li>Verify your LLM API keys are valid in Settings \u2192 LLM Keys</li> <li>Check that you have sufficient credits with your LLM provider</li> <li>Ensure the <code>model</code> parameter matches your provider (e.g., 'gpt-4o' for OpenAI)</li> </ul> <p>Need more help? Contact support</p>"},{"location":"guides/best-practices/","title":"Best Practices","text":"<p>Production-ready tips for deploying Vectorcache in your application.</p>"},{"location":"guides/best-practices/#security","title":"Security","text":""},{"location":"guides/best-practices/#1-protect-api-keys","title":"1. Protect API Keys","text":"<p>\u274c Never hardcode API keys:</p> <pre><code>// DON'T DO THIS\nconst apiKey = 'vc_1234567890abcdef';\n</code></pre> <p>\u2705 Use environment variables:</p> <pre><code>// DO THIS\nconst apiKey = process.env.VECTORCACHE_API_KEY;\n</code></pre>"},{"location":"guides/best-practices/#2-separate-keys-per-environment","title":"2. Separate Keys per Environment","text":"<p>Use different API keys for development, staging, and production:</p> <pre><code># .env.development\nVECTORCACHE_API_KEY=vc_dev_key...\n\n# .env.staging\nVECTORCACHE_API_KEY=vc_staging_key...\n\n# .env.production\nVECTORCACHE_API_KEY=vc_prod_key...\n</code></pre>"},{"location":"guides/best-practices/#3-rotate-keys-regularly","title":"3. Rotate Keys Regularly","text":"<ul> <li>Create new keys every 90 days</li> <li>Revoke old keys after migration</li> <li>Monitor key usage in dashboard</li> </ul>"},{"location":"guides/best-practices/#4-never-commit-keys-to-git","title":"4. Never Commit Keys to Git","text":"<p>Add to <code>.gitignore</code>:</p> <pre><code>.env\n.env.local\n.env.*.local\nconfig/secrets.json\n</code></pre>"},{"location":"guides/best-practices/#error-handling","title":"Error Handling","text":""},{"location":"guides/best-practices/#1-always-handle-both-cache-hit-and-miss","title":"1. Always Handle Both Cache Hit and Miss","text":"<pre><code>const result = await client.query({ prompt, model });\n\nif (result.cache_hit) {\n  console.log(`Cache hit! Saved $${result.cost_saved}`);\n} else {\n  console.log('Cache miss - stored for future use');\n}\n\n// Your app should work regardless of cache_hit value\nreturn result.response;\n</code></pre>"},{"location":"guides/best-practices/#2-implement-retry-logic","title":"2. Implement Retry Logic","text":"<pre><code>async function queryWithRetry(request, maxRetries = 3) {\n  for (let attempt = 1; attempt &lt;= maxRetries; attempt++) {\n    try {\n      return await client.query(request);\n    } catch (error) {\n      if (error.statusCode === 429 &amp;&amp; attempt &lt; maxRetries) {\n        const delay = Math.min(1000 * Math.pow(2, attempt), 10000);\n        await new Promise(resolve =&gt; setTimeout(resolve, delay));\n        continue;\n      }\n      throw error;\n    }\n  }\n}\n</code></pre>"},{"location":"guides/best-practices/#3-graceful-degradation","title":"3. Graceful Degradation","text":"<p>Have a fallback when Vectorcache is unavailable:</p> <pre><code>async function getResponse(prompt: string) {\n  try {\n    const result = await client.query({ prompt, model: 'gpt-4o' });\n    return result.response;\n  } catch (error) {\n    console.error('Vectorcache error, using fallback:', error);\n    return await directLLMCall(prompt);\n  }\n}\n</code></pre>"},{"location":"guides/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/best-practices/#1-set-appropriate-timeouts","title":"1. Set Appropriate Timeouts","text":"<pre><code>const client = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!,\n  timeout: 10000 // 10 seconds for production\n});\n</code></pre>"},{"location":"guides/best-practices/#2-use-connection-pooling","title":"2. Use Connection Pooling","text":"<p>For high-throughput applications:</p> <pre><code># Python example with connection pooling\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom requests.packages.urllib3.util.retry import Retry\n\nsession = requests.Session()\nretry = Retry(total=3, backoff_factor=0.3)\nadapter = HTTPAdapter(max_retries=retry, pool_maxsize=100)\nsession.mount('https://', adapter)\n</code></pre>"},{"location":"guides/best-practices/#3-batch-similar-requests","title":"3. Batch Similar Requests","text":"<p>Group related queries together:</p> <pre><code>const prompts = [\n  'What is ML?',\n  'Explain deep learning',\n  'What are neural networks?'\n];\n\nconst results = await Promise.all(\n  prompts.map(prompt =&gt; client.query({ prompt, model: 'gpt-4o' }))\n);\n</code></pre>"},{"location":"guides/best-practices/#4-monitor-cache-performance","title":"4. Monitor Cache Performance","text":"<p>Track metrics to optimize threshold:</p> <pre><code>let totalQueries = 0;\nlet cacheHits = 0;\n\nasync function trackQuery(prompt: string) {\n  const result = await client.query({ prompt, model: 'gpt-4o' });\n\n  totalQueries++;\n  if (result.cache_hit) cacheHits++;\n\n  const hitRate = (cacheHits / totalQueries * 100).toFixed(1);\n  console.log(`Cache hit rate: ${hitRate}%`);\n\n  return result;\n}\n</code></pre>"},{"location":"guides/best-practices/#caching-strategy","title":"Caching Strategy","text":""},{"location":"guides/best-practices/#1-use-context-for-segmentation","title":"1. Use Context for Segmentation","text":"<p>Separate caches by use case:</p> <pre><code>// Customer support queries\nawait client.query({\n  prompt: 'How do I reset my password?',\n  context: 'customer-support',\n  model: 'gpt-4o'\n});\n\n// Internal documentation\nawait client.query({\n  prompt: 'How do I reset my password?',\n  context: 'internal-docs',\n  model: 'gpt-4o'\n});\n</code></pre>"},{"location":"guides/best-practices/#2-choose-similarity-thresholds-wisely","title":"2. Choose Similarity Thresholds Wisely","text":"Use Case Recommended Threshold Legal/Medical 0.92-0.95 Customer Support 0.85-0.90 Education/FAQs 0.80-0.85 General Content 0.85"},{"location":"guides/best-practices/#3-test-thresholds-with-real-data","title":"3. Test Thresholds with Real Data","text":"<pre><code>const thresholds = [0.80, 0.85, 0.90, 0.95];\n\nfor (const threshold of thresholds) {\n  const result = await client.query({\n    prompt: testPrompt,\n    model: 'gpt-4o',\n    similarityThreshold: threshold\n  });\n\n  console.log(`Threshold ${threshold}: ${result.cache_hit ? 'HIT' : 'MISS'}`);\n  if (result.similarity_score) {\n    console.log(`Score: ${result.similarity_score}`);\n  }\n}\n</code></pre>"},{"location":"guides/best-practices/#monitoring","title":"Monitoring","text":""},{"location":"guides/best-practices/#1-track-key-metrics","title":"1. Track Key Metrics","text":"<p>Monitor these metrics in your application:</p> <pre><code>interface CacheMetrics {\n  totalQueries: number;\n  cacheHits: number;\n  cacheMisses: number;\n  totalCostSaved: number;\n  averageSimilarityScore: number;\n  errorCount: number;\n}\n\nconst metrics: CacheMetrics = {\n  totalQueries: 0,\n  cacheHits: 0,\n  cacheMisses: 0,\n  totalCostSaved: 0,\n  averageSimilarityScore: 0,\n  errorCount: 0\n};\n\nasync function queryAndTrack(request) {\n  try {\n    const result = await client.query(request);\n\n    metrics.totalQueries++;\n    if (result.cache_hit) {\n      metrics.cacheHits++;\n      metrics.totalCostSaved += result.cost_saved;\n    } else {\n      metrics.cacheMisses++;\n    }\n\n    return result;\n  } catch (error) {\n    metrics.errorCount++;\n    throw error;\n  }\n}\n</code></pre>"},{"location":"guides/best-practices/#2-log-important-events","title":"2. Log Important Events","text":"<pre><code>function logCacheEvent(result: CacheQueryResponse, request: CacheQueryRequest) {\n  const logEntry = {\n    timestamp: new Date().toISOString(),\n    cache_hit: result.cache_hit,\n    similarity_score: result.similarity_score,\n    cost_saved: result.cost_saved,\n    model: request.model,\n    context: request.context,\n    threshold: request.similarityThreshold\n  };\n\n  logger.info('cache_query', logEntry);\n\n  // Alert on low hit rates\n  const hitRate = metrics.cacheHits / metrics.totalQueries;\n  if (hitRate &lt; 0.3 &amp;&amp; metrics.totalQueries &gt; 100) {\n    logger.warn('Low cache hit rate', { hitRate, metrics });\n  }\n}\n</code></pre>"},{"location":"guides/best-practices/#3-set-up-alerts","title":"3. Set Up Alerts","text":"<p>Alert on important conditions:</p> <pre><code>function checkAndAlert(metrics: CacheMetrics) {\n  // High error rate\n  const errorRate = metrics.errorCount / metrics.totalQueries;\n  if (errorRate &gt; 0.05) {\n    alert('High Vectorcache error rate', { errorRate, metrics });\n  }\n\n  // Low hit rate\n  const hitRate = metrics.cacheHits / metrics.totalQueries;\n  if (hitRate &lt; 0.3 &amp;&amp; metrics.totalQueries &gt; 100) {\n    alert('Low cache hit rate', { hitRate, metrics });\n  }\n\n  // High cost (check if caching is actually saving money)\n  const expectedSavings = metrics.totalQueries * 0.003; // Assume $0.003 per query\n  if (metrics.totalCostSaved &lt; expectedSavings * 0.5) {\n    alert('Lower than expected cost savings', { metrics });\n  }\n}\n</code></pre>"},{"location":"guides/best-practices/#testing","title":"Testing","text":""},{"location":"guides/best-practices/#1-unit-tests","title":"1. Unit Tests","text":"<p>Test your Vectorcache integration:</p> <pre><code>import { VectorcacheClient } from 'vectorcache';\n\ndescribe('Vectorcache Integration', () =&gt; {\n  const client = new VectorcacheClient({\n    apiKey: process.env.TEST_VECTORCACHE_KEY!\n  });\n\n  it('should handle cache hits', async () =&gt; {\n    const prompt = 'Test prompt for caching';\n\n    // First call - cache miss\n    const result1 = await client.query({ prompt, model: 'gpt-4o' });\n    expect(result1.cache_hit).toBe(false);\n\n    // Second call - cache hit\n    const result2 = await client.query({ prompt, model: 'gpt-4o' });\n    expect(result2.cache_hit).toBe(true);\n    expect(result2.similarity_score).toBeGreaterThan(0.9);\n  });\n\n  it('should handle errors gracefully', async () =&gt; {\n    const invalidClient = new VectorcacheClient({ apiKey: 'invalid' });\n\n    await expect(\n      invalidClient.query({ prompt: 'test', model: 'gpt-4o' })\n    ).rejects.toThrow('Invalid API key');\n  });\n});\n</code></pre>"},{"location":"guides/best-practices/#2-load-testing","title":"2. Load Testing","text":"<p>Test with realistic load:</p> <pre><code>async function loadTest(concurrency: number, totalRequests: number) {\n  const results = [];\n  const batchSize = concurrency;\n\n  for (let i = 0; i &lt; totalRequests; i += batchSize) {\n    const batch = Array(Math.min(batchSize, totalRequests - i))\n      .fill(null)\n      .map(() =&gt; client.query({\n        prompt: `Test query ${i}`,\n        model: 'gpt-4o'\n      }));\n\n    const batchResults = await Promise.all(batch);\n    results.push(...batchResults);\n  }\n\n  const hitRate = results.filter(r =&gt; r.cache_hit).length / results.length;\n  console.log(`Load test complete: ${hitRate * 100}% hit rate`);\n}\n</code></pre>"},{"location":"guides/best-practices/#cost-optimization","title":"Cost Optimization","text":""},{"location":"guides/best-practices/#1-track-roi","title":"1. Track ROI","text":"<p>Monitor actual cost savings:</p> <pre><code>interface CostAnalysis {\n  totalQueries: number;\n  cacheHits: number;\n  totalCostSaved: number;\n  vectorcacheCost: number; // Your Vectorcache subscription\n  netSavings: number;\n}\n\nfunction analyzeCosts(metrics: CacheMetrics): CostAnalysis {\n  const vectorcacheCost = 29; // Example: $29/month plan\n\n  return {\n    totalQueries: metrics.totalQueries,\n    cacheHits: metrics.cacheHits,\n    totalCostSaved: metrics.totalCostSaved,\n    vectorcacheCost,\n    netSavings: metrics.totalCostSaved - vectorcacheCost\n  };\n}\n</code></pre>"},{"location":"guides/best-practices/#2-optimize-threshold-based-on-roi","title":"2. Optimize Threshold Based on ROI","text":"<pre><code>function findOptimalThreshold(testPrompts: string[]) {\n  const thresholds = [0.75, 0.80, 0.85, 0.90, 0.95];\n  const results = [];\n\n  for (const threshold of thresholds) {\n    let hits = 0;\n    let totalSaved = 0;\n\n    for (const prompt of testPrompts) {\n      const result = await client.query({\n        prompt,\n        model: 'gpt-4o',\n        similarityThreshold: threshold\n      });\n\n      if (result.cache_hit) {\n        hits++;\n        totalSaved += result.cost_saved;\n      }\n    }\n\n    results.push({\n      threshold,\n      hitRate: hits / testPrompts.length,\n      totalSaved\n    });\n  }\n\n  // Find threshold with best ROI\n  return results.sort((a, b) =&gt; b.totalSaved - a.totalSaved)[0];\n}\n</code></pre>"},{"location":"guides/best-practices/#production-checklist","title":"Production Checklist","text":"<p>Before going live:</p> <ul> <li>[ ] API keys stored in environment variables</li> <li>[ ] Separate keys for dev/staging/production</li> <li>[ ] Error handling implemented with retries</li> <li>[ ] Fallback logic for service unavailability</li> <li>[ ] Timeouts configured appropriately</li> <li>[ ] Similarity threshold tested with real data</li> <li>[ ] Context used for cache segmentation</li> <li>[ ] Metrics and logging implemented</li> <li>[ ] Alerts configured for errors and low hit rates</li> <li>[ ] Load testing completed</li> <li>[ ] Cost analysis reviewed</li> <li>[ ] <code>.env</code> files in <code>.gitignore</code></li> </ul>"},{"location":"guides/best-practices/#next-steps","title":"Next Steps","text":"<ul> <li>Similarity Tuning Guide - Optimize cache performance</li> <li>Cost Optimization - Maximize savings</li> <li>Security Guide - Advanced security practices</li> </ul>"},{"location":"guides/cost-optimization/","title":"Cost Optimization","text":"<p>Maximize your ROI with Vectorcache by optimizing cost savings.</p>"},{"location":"guides/cost-optimization/#understanding-costs","title":"Understanding Costs","text":""},{"location":"guides/cost-optimization/#llm-api-costs","title":"LLM API Costs","text":"<p>Typical LLM API costs per query:</p> Model Cost per Query Annual Cost (1000 queries/day) GPT-4o $0.002-0.005 $730-1,825 GPT-4o-mini $0.0003-0.001 $110-365 Claude 3.5 Sonnet $0.003-0.008 $1,095-2,920 Claude 3.5 Haiku $0.0005-0.001 $183-365"},{"location":"guides/cost-optimization/#vectorcache-savings","title":"Vectorcache Savings","text":"<p>With 50% cache hit rate:</p> Model Without Cache With Vectorcache (50% hits) Annual Savings GPT-4o $1,825/year $913/year $912/year Claude 3.5 Sonnet $2,920/year $1,460/year $1,460/year <p>Assuming $29/month Vectorcache subscription</p>"},{"location":"guides/cost-optimization/#calculating-roi","title":"Calculating ROI","text":""},{"location":"guides/cost-optimization/#formula","title":"Formula","text":"<pre><code>ROI = (Cost Saved - Vectorcache Cost) / Vectorcache Cost \u00d7 100%\n\nCost Saved = (Cache Hits \u00d7 Avg Query Cost)\n</code></pre>"},{"location":"guides/cost-optimization/#example-calculation","title":"Example Calculation","text":"<pre><code>interface ROICalculation {\n  monthlyQueries: number;\n  cacheHitRate: number;\n  avgQueryCost: number;\n  vectorcacheCost: number;\n}\n\nfunction calculateROI(params: ROICalculation): number {\n  const cacheHits = params.monthlyQueries * params.cacheHitRate;\n  const costSaved = cacheHits * params.avgQueryCost;\n  const netSavings = costSaved - params.vectorcacheCost;\n  const roi = (netSavings / params.vectorcacheCost) * 100;\n\n  return roi;\n}\n\n// Example: 30,000 queries/month, 50% hit rate, $0.003/query, $29/month\nconst roi = calculateROI({\n  monthlyQueries: 30000,\n  cacheHitRate: 0.50,\n  avgQueryCost: 0.003,\n  vectorcacheCost: 29\n});\n\nconsole.log(`ROI: ${roi.toFixed(0)}%`); // ~55% ROI\n</code></pre>"},{"location":"guides/cost-optimization/#maximizing-cache-hit-rate","title":"Maximizing Cache Hit Rate","text":""},{"location":"guides/cost-optimization/#1-optimize-similarity-threshold","title":"1. Optimize Similarity Threshold","text":"<p>Lower threshold = Higher hit rate (but less accuracy):</p> <pre><code>// Test different thresholds\nconst thresholds = [0.80, 0.85, 0.90];\n\nfor (const threshold of thresholds) {\n  const metrics = await testThreshold(threshold, testQueries);\n  console.log(`Threshold ${threshold}: ${metrics.hitRate}% hits, $${metrics.saved}`);\n}\n\n// Choose threshold with best ROI\n</code></pre>"},{"location":"guides/cost-optimization/#2-use-context-segmentation","title":"2. Use Context Segmentation","text":"<p>Segment cache by use case for better matches:</p> <pre><code>// Separate caches for different contexts\nawait client.query({\n  prompt: 'Reset password',\n  context: 'customer-support-auth',\n  model: 'gpt-4o'\n});\n\nawait client.query({\n  prompt: 'Reset password',\n  context: 'admin-documentation',\n  model: 'gpt-4o'\n});\n</code></pre>"},{"location":"guides/cost-optimization/#3-normalize-user-input","title":"3. Normalize User Input","text":"<p>Preprocess queries for better matching:</p> <pre><code>function normalizePrompt(prompt: string): string {\n  return prompt\n    .toLowerCase()\n    .trim()\n    .replace(/[^\\w\\s]/g, ' ')  // Remove punctuation\n    .replace(/\\s+/g, ' ');     // Normalize whitespace\n}\n\nconst result = await client.query({\n  prompt: normalizePrompt(userInput),\n  model: 'gpt-4o'\n});\n</code></pre>"},{"location":"guides/cost-optimization/#cost-tracking","title":"Cost Tracking","text":""},{"location":"guides/cost-optimization/#track-actual-savings","title":"Track Actual Savings","text":"<pre><code>class CostTracker {\n  private totalSaved = 0;\n  private totalQueries = 0;\n  private cacheHits = 0;\n\n  async query(request: CacheQueryRequest) {\n    const result = await client.query(request);\n\n    this.totalQueries++;\n    if (result.cache_hit) {\n      this.cacheHits++;\n      this.totalSaved += result.cost_saved;\n    }\n\n    return result;\n  }\n\n  getMetrics() {\n    const hitRate = (this.cacheHits / this.totalQueries * 100).toFixed(1);\n    const avgSaved = this.totalSaved / this.cacheHits;\n\n    return {\n      totalQueries: this.totalQueries,\n      cacheHits: this.cacheHits,\n      hitRate: `${hitRate}%`,\n      totalSaved: `$${this.totalSaved.toFixed(2)}`,\n      avgSavedPerHit: `$${avgSaved.toFixed(4)}`\n    };\n  }\n}\n</code></pre>"},{"location":"guides/cost-optimization/#monthly-cost-analysis","title":"Monthly Cost Analysis","text":"<pre><code>function analyzeMonthCosts(metrics: CacheMetrics) {\n  const vectorcacheCost = 29; // Monthly subscription\n  const costSaved = metrics.totalCostSaved;\n  const netSavings = costSaved - vectorcacheCost;\n  const roi = (netSavings / vectorcacheCost) * 100;\n\n  return {\n    vectorcacheCost: `$${vectorcacheCost}`,\n    llmCostSaved: `$${costSaved.toFixed(2)}`,\n    netSavings: `$${netSavings.toFixed(2)}`,\n    roi: `${roi.toFixed(0)}%`,\n    breakEven: netSavings &gt;= 0\n  };\n}\n</code></pre>"},{"location":"guides/cost-optimization/#when-vectorcache-makes-sense","title":"When Vectorcache Makes Sense","text":""},{"location":"guides/cost-optimization/#great-fit","title":"\u2705 Great Fit","text":"<ul> <li>High query volume: 10,000+ queries/month</li> <li>Repetitive queries: Customer support, FAQs, documentation</li> <li>Expensive models: GPT-4o, Claude 3.5 Sonnet</li> <li>Similar user questions: Educational platforms, chatbots</li> </ul>"},{"location":"guides/cost-optimization/#may-not-be-worth-it","title":"\u26a0\ufe0f May Not Be Worth It","text":"<ul> <li>Low query volume: &lt;1,000 queries/month</li> <li>Unique queries: Each query is completely different</li> <li>Cheap models only: Using only GPT-4o-mini or similar</li> <li>Real-time data: Queries require latest information</li> </ul>"},{"location":"guides/cost-optimization/#break-even-analysis","title":"Break-Even Analysis","text":"<p>Minimum queries needed to break even (at different hit rates):</p> Model Cost/Query 30% Hit Rate 50% Hit Rate 70% Hit Rate GPT-4o ($0.003) $0.003 ~32,000 ~19,000 ~14,000 GPT-4o-mini ($0.0005) $0.0005 ~193,000 ~116,000 ~83,000 Claude 3.5 Sonnet ($0.005) $0.005 ~19,000 ~12,000 ~8,000 <p>Monthly queries needed to break even at $29/month</p>"},{"location":"guides/cost-optimization/#cost-optimization-strategies","title":"Cost Optimization Strategies","text":""},{"location":"guides/cost-optimization/#1-use-cheaper-models-for-cache-misses","title":"1. Use Cheaper Models for Cache Misses","text":"<pre><code>async function smartQuery(prompt: string) {\n  // Try cache first\n  const result = await client.query({\n    prompt,\n    model: 'gpt-4o',\n    similarityThreshold: 0.85\n  });\n\n  // If cache miss and query is simple, use cheaper model\n  if (!result.cache_hit &amp;&amp; isSimpleQuery(prompt)) {\n    return await client.query({\n      prompt,\n      model: 'gpt-4o-mini', // Cheaper alternative\n      similarityThreshold: 0.85\n    });\n  }\n\n  return result;\n}\n</code></pre>"},{"location":"guides/cost-optimization/#2-batch-similar-queries","title":"2. Batch Similar Queries","text":"<p>Group related queries to maximize cache hits:</p> <pre><code>async function batchQuery(prompts: string[]) {\n  // Group similar prompts\n  const groups = groupSimilarPrompts(prompts);\n\n  // Query each group once, reuse for similar prompts\n  const results = [];\n  for (const group of groups) {\n    const result = await client.query({\n      prompt: group[0], // Use first as representative\n      model: 'gpt-4o'\n    });\n\n    // Reuse result for all similar prompts\n    group.forEach(prompt =&gt; {\n      results.push({ prompt, result });\n    });\n  }\n\n  return results;\n}\n</code></pre>"},{"location":"guides/cost-optimization/#3-implement-smart-caching-logic","title":"3. Implement Smart Caching Logic","text":"<pre><code>async function intelligentCache(prompt: string, userContext: any) {\n  // Don't cache unique/time-sensitive queries\n  if (isTimeSensitive(prompt) || isUserSpecific(prompt, userContext)) {\n    return await directLLMCall(prompt);\n  }\n\n  // Use cache for general queries\n  return await client.query({\n    prompt,\n    model: 'gpt-4o',\n    similarityThreshold: 0.85\n  });\n}\n\nfunction isTimeSensitive(prompt: string): boolean {\n  const timeKeywords = ['today', 'now', 'current', 'latest', 'recent'];\n  return timeKeywords.some(kw =&gt; prompt.toLowerCase().includes(kw));\n}\n</code></pre>"},{"location":"guides/cost-optimization/#monitoring-roi","title":"Monitoring ROI","text":""},{"location":"guides/cost-optimization/#dashboard-metrics","title":"Dashboard Metrics","text":"<p>Track these metrics in your dashboard:</p> <pre><code>interface ROIDashboard {\n  period: string;\n  totalQueries: number;\n  cacheHits: number;\n  hitRate: string;\n  llmCostSaved: string;\n  vectorcacheCost: string;\n  netSavings: string;\n  roi: string;\n}\n\nfunction generateROIDashboard(metrics: CacheMetrics): ROIDashboard {\n  const vectorcacheCost = 29;\n  const netSavings = metrics.totalCostSaved - vectorcacheCost;\n  const roi = (netSavings / vectorcacheCost) * 100;\n\n  return {\n    period: 'This Month',\n    totalQueries: metrics.totalQueries,\n    cacheHits: metrics.cacheHits,\n    hitRate: `${(metrics.cacheHits / metrics.totalQueries * 100).toFixed(1)}%`,\n    llmCostSaved: `$${metrics.totalCostSaved.toFixed(2)}`,\n    vectorcacheCost: `$${vectorcacheCost}`,\n    netSavings: `$${netSavings.toFixed(2)}`,\n    roi: `${roi.toFixed(0)}%`\n  };\n}\n</code></pre>"},{"location":"guides/cost-optimization/#alerts-for-poor-roi","title":"Alerts for Poor ROI","text":"<pre><code>function checkROI(metrics: CacheMetrics) {\n  const vectorcacheCost = 29;\n  const netSavings = metrics.totalCostSaved - vectorcacheCost;\n\n  if (netSavings &lt; 0 &amp;&amp; metrics.totalQueries &gt; 1000) {\n    alert('Negative ROI detected', {\n      netSavings: `$${netSavings.toFixed(2)}`,\n      suggestion: 'Consider lowering similarity threshold or increasing query volume'\n    });\n  }\n}\n</code></pre>"},{"location":"guides/cost-optimization/#real-world-examples","title":"Real-World Examples","text":""},{"location":"guides/cost-optimization/#example-1-customer-support-chatbot","title":"Example 1: Customer Support Chatbot","text":"<p>Scenario: - 50,000 queries/month - 60% cache hit rate - GPT-4o ($0.003/query) - $29/month Vectorcache</p> <p>Calculation: <pre><code>Cache hits: 50,000 \u00d7 0.60 = 30,000\nCost saved: 30,000 \u00d7 $0.003 = $90\nNet savings: $90 - $29 = $61/month\nROI: ($61 / $29) \u00d7 100 = 210%\nAnnual savings: $732\n</code></pre></p>"},{"location":"guides/cost-optimization/#example-2-educational-platform","title":"Example 2: Educational Platform","text":"<p>Scenario: - 100,000 queries/month - 70% cache hit rate (high due to repetitive educational questions) - GPT-4o ($0.003/query) - $29/month Vectorcache</p> <p>Calculation: <pre><code>Cache hits: 100,000 \u00d7 0.70 = 70,000\nCost saved: 70,000 \u00d7 $0.003 = $210\nNet savings: $210 - $29 = $181/month\nROI: ($181 / $29) \u00d7 100 = 624%\nAnnual savings: $2,172\n</code></pre></p>"},{"location":"guides/cost-optimization/#best-practices","title":"Best Practices","text":"<ol> <li>Track metrics religiously - Know your exact hit rate and cost savings</li> <li>Test thresholds - Find the sweet spot for your use case</li> <li>Segment caches - Use context for better organization</li> <li>Monitor ROI - Alert when ROI drops below acceptable level</li> <li>Optimize continuously - Adjust based on real data</li> </ol>"},{"location":"guides/cost-optimization/#next-steps","title":"Next Steps","text":"<ul> <li>Best Practices - Production tips</li> <li>Similarity Tuning - Optimize hit rate</li> <li>API Reference - API documentation</li> </ul>"},{"location":"guides/error-handling/","title":"Error Handling Best Practices","text":"<p>Learn how to properly handle errors when integrating Vectorcache into your application to ensure resilient, production-ready implementations.</p>"},{"location":"guides/error-handling/#overview","title":"Overview","text":"<p>Vectorcache returns standard HTTP status codes to indicate the success or failure of API requests. Your application should implement proper error handling to gracefully handle failures and provide fallback behavior when the cache service is unavailable.</p>"},{"location":"guides/error-handling/#http-status-codes","title":"HTTP Status Codes","text":"<p>Vectorcache uses the following HTTP status codes:</p> Status Code Meaning When It Happens What To Do 200 Success Request processed successfully Use the response data 400 Bad Request Malformed request body or invalid parameters Fix the request format 401 Unauthorized Invalid or missing API key Check your API key 422 Unprocessable Entity Content not cacheable (images, PDFs, streaming, etc.) Call LLM directly 429 Too Many Requests Rate limit exceeded (monthly quota) Wait or upgrade plan 500 Internal Server Error Unexpected server error Retry with exponential backoff 503 Service Unavailable Database or service outage Call LLM directly, check Retry-After header"},{"location":"guides/error-handling/#error-response-format","title":"Error Response Format","text":"<p>All error responses follow this structure:</p> <pre><code>{\n  \"error\": \"Short error type\",\n  \"message\": \"Human-readable error description\",\n  \"retry_after\": 60,  // (Optional) Seconds to wait before retry\n  \"fallback\": \"Suggested fallback action\"  // (Optional)\n}\n</code></pre>"},{"location":"guides/error-handling/#javascripttypescript-error-handling","title":"JavaScript/TypeScript Error Handling","text":""},{"location":"guides/error-handling/#complete-example-with-fallback","title":"Complete Example with Fallback","text":"<pre><code>import { VectorcacheClient, VectorcacheError } from 'vectorcache';\nimport OpenAI from 'openai';\n\nconst vectorcache = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!,\n  baseURL: 'https://api.vectorcache.com'\n});\n\nconst openai = new OpenAI({\n  apiKey: process.env.OPENAI_API_KEY!\n});\n\nasync function getChatResponse(prompt: string, context: string = ''): Promise&lt;string&gt; {\n  try {\n    // Try Vectorcache first\n    const result = await vectorcache.query({\n      prompt: prompt,\n      context: context,\n      model: 'gpt-4',\n      similarity_threshold: 0.85\n    });\n\n    if (result.cache_hit) {\n      console.log('\u2705 Cache hit! Saved time and money');\n    } else {\n      console.log('\ud83d\udcdd Cache miss - new response generated');\n    }\n\n    return result.response;\n\n  } catch (error: any) {\n    return handleVectorcacheError(error, prompt, context);\n  }\n}\n\nasync function handleVectorcacheError(\n  error: any,\n  prompt: string,\n  context: string\n): Promise&lt;string&gt; {\n  const status = error.response?.status;\n\n  switch (status) {\n    case 503:\n      // Service unavailable - use fallback\n      console.warn('\u26a0\ufe0f Vectorcache temporarily unavailable, calling LLM directly');\n      const retryAfter = error.response?.headers['retry-after'] || 60;\n      console.log(`   Retry after ${retryAfter} seconds`);\n      return callLLMDirectly(prompt, context);\n\n    case 422:\n      // Content not cacheable\n      console.warn('\u26a0\ufe0f Content not cacheable (may contain images/PDFs), calling LLM directly');\n      return callLLMDirectly(prompt, context);\n\n    case 429:\n      // Rate limit exceeded\n      console.error('\u274c Monthly Vectorcache limit exceeded');\n      const detail = error.response?.data;\n      console.error(`   Usage: ${detail.current_usage}/${detail.monthly_limit}`);\n      throw new Error('Vectorcache monthly limit reached. Please upgrade your plan.');\n\n    case 401:\n      // Invalid API key\n      console.error('\u274c Invalid Vectorcache API key');\n      throw new Error('Vectorcache authentication failed. Check your API key.');\n\n    case 500:\n      // Internal server error - retry with exponential backoff\n      console.error('\u274c Vectorcache internal error, retrying...');\n      await sleep(1000);\n      try {\n        return await getChatResponse(prompt, context);\n      } catch {\n        // Retry failed, fallback to direct LLM\n        return callLLMDirectly(prompt, context);\n      }\n\n    default:\n      // Unexpected error - fallback\n      console.error('\u274c Unexpected Vectorcache error:', error.message);\n      return callLLMDirectly(prompt, context);\n  }\n}\n\nasync function callLLMDirectly(prompt: string, context: string = ''): Promise&lt;string&gt; {\n  /**\n   * Fallback: Call OpenAI directly when Vectorcache is unavailable\n   */\n  console.log('\ud83d\udd04 Falling back to direct OpenAI call');\n\n  const messages: any[] = [];\n  if (context) {\n    messages.push({ role: 'system', content: context });\n  }\n  messages.push({ role: 'user', content: prompt });\n\n  const response = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: messages\n  });\n\n  return response.choices[0].message.content || '';\n}\n\nfunction sleep(ms: number): Promise&lt;void&gt; {\n  return new Promise(resolve =&gt; setTimeout(resolve, ms));\n}\n\n// Usage\nasync function main() {\n  const response = await getChatResponse(\n    'What are the benefits of semantic caching?',\n    'You are a helpful AI assistant'\n  );\n  console.log('Response:', response);\n}\n\nmain();\n</code></pre>"},{"location":"guides/error-handling/#python-error-handling","title":"Python Error Handling","text":""},{"location":"guides/error-handling/#complete-example-with-fallback_1","title":"Complete Example with Fallback","text":"<pre><code>import os\nimport time\nfrom typing import Optional\nfrom vectorcache import VectorcacheClient, VectorcacheError\nfrom openai import OpenAI\n\nvectorcache = VectorcacheClient(\n    api_key=os.getenv(\"VECTORCACHE_API_KEY\"),\n    base_url=\"https://api.vectorcache.com\"\n)\n\nopenai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n\ndef get_chat_response(prompt: str, context: str = \"\") -&gt; str:\n    \"\"\"\n    Get chat response with Vectorcache, falling back to direct LLM on error.\n    \"\"\"\n    try:\n        # Try Vectorcache first\n        result = vectorcache.query(\n            prompt=prompt,\n            context=context,\n            model=\"gpt-4\",\n            similarity_threshold=0.85\n        )\n\n        if result['cache_hit']:\n            print('\u2705 Cache hit! Saved time and money')\n        else:\n            print('\ud83d\udcdd Cache miss - new response generated')\n\n        return result['response']\n\n    except VectorcacheError as e:\n        return handle_vectorcache_error(e, prompt, context)\n\n\ndef handle_vectorcache_error(error: VectorcacheError, prompt: str, context: str = \"\") -&gt; str:\n    \"\"\"\n    Handle Vectorcache errors with appropriate fallback strategies.\n    \"\"\"\n    status = error.status_code\n\n    if status == 503:\n        # Service unavailable - use fallback\n        print('\u26a0\ufe0f Vectorcache temporarily unavailable, calling LLM directly')\n        retry_after = error.retry_after or 60\n        print(f'   Retry after {retry_after} seconds')\n        return call_llm_directly(prompt, context)\n\n    elif status == 422:\n        # Content not cacheable\n        print('\u26a0\ufe0f Content not cacheable (may contain images/PDFs), calling LLM directly')\n        return call_llm_directly(prompt, context)\n\n    elif status == 429:\n        # Rate limit exceeded\n        print(f'\u274c Monthly Vectorcache limit exceeded: {error.message}')\n        raise Exception('Vectorcache monthly limit reached. Please upgrade your plan.')\n\n    elif status == 401:\n        # Invalid API key\n        print('\u274c Invalid Vectorcache API key')\n        raise Exception('Vectorcache authentication failed. Check your API key.')\n\n    elif status == 500:\n        # Internal server error - retry once with exponential backoff\n        print('\u274c Vectorcache internal error, retrying...')\n        time.sleep(1)\n        try:\n            return get_chat_response(prompt, context)\n        except:\n            # Retry failed, fallback to direct LLM\n            return call_llm_directly(prompt, context)\n\n    else:\n        # Unexpected error - fallback\n        print(f'\u274c Unexpected Vectorcache error: {error.message}')\n        return call_llm_directly(prompt, context)\n\n\ndef call_llm_directly(prompt: str, context: str = \"\") -&gt; str:\n    \"\"\"\n    Fallback: Call OpenAI directly when Vectorcache is unavailable.\n    \"\"\"\n    print('\ud83d\udd04 Falling back to direct OpenAI call')\n\n    messages = []\n    if context:\n        messages.append({\"role\": \"system\", \"content\": context})\n    messages.append({\"role\": \"user\", \"content\": prompt})\n\n    response = openai_client.chat.completions.create(\n        model=\"gpt-4\",\n        messages=messages\n    )\n\n    return response.choices[0].message.content\n\n\n# Usage\nif __name__ == \"__main__\":\n    response = get_chat_response(\n        prompt=\"What are the benefits of semantic caching?\",\n        context=\"You are a helpful AI assistant\"\n    )\n    print(f\"Response: {response}\")\n</code></pre>"},{"location":"guides/error-handling/#retry-strategies","title":"Retry Strategies","text":""},{"location":"guides/error-handling/#exponential-backoff","title":"Exponential Backoff","text":"<p>For 500/503 errors, implement exponential backoff:</p> <pre><code>async function retryWithBackoff&lt;T&gt;(\n  fn: () =&gt; Promise&lt;T&gt;,\n  maxRetries: number = 3,\n  initialDelay: number = 1000\n): Promise&lt;T&gt; {\n  let lastError: any;\n\n  for (let attempt = 0; attempt &lt; maxRetries; attempt++) {\n    try {\n      return await fn();\n    } catch (error: any) {\n      lastError = error;\n\n      // Don't retry on client errors (4xx)\n      if (error.response?.status &gt;= 400 &amp;&amp; error.response?.status &lt; 500) {\n        throw error;\n      }\n\n      // Calculate delay: 1s, 2s, 4s, 8s...\n      const delay = initialDelay * Math.pow(2, attempt);\n      console.log(`Retry attempt ${attempt + 1}/${maxRetries} in ${delay}ms`);\n\n      await new Promise(resolve =&gt; setTimeout(resolve, delay));\n    }\n  }\n\n  throw lastError;\n}\n\n// Usage\nconst result = await retryWithBackoff(() =&gt;\n  vectorcache.query({ prompt, context, model: 'gpt-4' })\n);\n</code></pre>"},{"location":"guides/error-handling/#respecting-retry-after-headers","title":"Respecting Retry-After Headers","text":"<p>When receiving 503 errors, always check the <code>Retry-After</code> header:</p> <pre><code>if (error.response?.status === 503) {\n  const retryAfter = parseInt(error.response.headers['retry-after'] || '60');\n  console.log(`Service unavailable. Retry after ${retryAfter} seconds`);\n\n  // Option 1: Wait and retry\n  await new Promise(resolve =&gt; setTimeout(resolve, retryAfter * 1000));\n  return await getChatResponse(prompt, context);\n\n  // Option 2: Fallback immediately\n  return callLLMDirectly(prompt, context);\n}\n</code></pre>"},{"location":"guides/error-handling/#timeout-configuration","title":"Timeout Configuration","text":"<p>Set appropriate timeouts to prevent hanging requests:</p> <pre><code>// JavaScript/TypeScript\nconst vectorcache = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!,\n  timeout: 10000  // 10 second timeout\n});\n</code></pre> <pre><code># Python\nimport httpx\n\nvectorcache = VectorcacheClient(\n    api_key=os.getenv(\"VECTORCACHE_API_KEY\"),\n    timeout=10.0  # 10 second timeout\n)\n</code></pre>"},{"location":"guides/error-handling/#health-check-monitoring","title":"Health Check Monitoring","text":"<p>Monitor Vectorcache health before making requests:</p> <pre><code>async function checkVectorcacheHealth(): Promise&lt;boolean&gt; {\n  try {\n    const response = await fetch('https://api.vectorcache.com/health/ready');\n    const data = await response.json();\n\n    if (response.status === 200 &amp;&amp; data.status === 'ready') {\n      return true;\n    }\n\n    console.warn('Vectorcache not ready:', data);\n    return false;\n  } catch (error) {\n    console.error('Vectorcache health check failed:', error);\n    return false;\n  }\n}\n\n// Usage\nconst isHealthy = await checkVectorcacheHealth();\nif (isHealthy) {\n  // Use Vectorcache\n  result = await vectorcache.query({...});\n} else {\n  // Skip cache, use LLM directly\n  result = await callLLMDirectly(prompt);\n}\n</code></pre>"},{"location":"guides/error-handling/#production-checklist","title":"Production Checklist","text":"<p>Before deploying to production, ensure you:</p> <ul> <li>\u2705 Implement try-catch blocks around all Vectorcache calls</li> <li>\u2705 Add fallback logic to call your LLM directly on errors</li> <li>\u2705 Set appropriate timeouts (recommended: 10 seconds)</li> <li>\u2705 Log errors for monitoring and debugging</li> <li>\u2705 Respect Retry-After headers for 503 errors</li> <li>\u2705 Implement exponential backoff for retries</li> <li>\u2705 Handle 422 errors (non-cacheable content) gracefully</li> <li>\u2705 Monitor rate limits (429 errors) and alert users</li> <li>\u2705 Test error scenarios in staging environment</li> </ul>"},{"location":"guides/error-handling/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/error-handling/#pattern-1-always-fallback","title":"Pattern 1: Always Fallback","text":"<pre><code>async function getChatResponse(prompt: string): Promise&lt;string&gt; {\n  try {\n    const result = await vectorcache.query({...});\n    return result.response;\n  } catch {\n    // Any error: fallback to LLM\n    return await callLLMDirectly(prompt);\n  }\n}\n</code></pre>"},{"location":"guides/error-handling/#pattern-2-fail-on-rate-limits","title":"Pattern 2: Fail on Rate Limits","text":"<pre><code>async function getChatResponse(prompt: string): Promise&lt;string&gt; {\n  try {\n    const result = await vectorcache.query({...});\n    return result.response;\n  } catch (error: any) {\n    if (error.response?.status === 429) {\n      // Don't fallback on rate limits - force user to upgrade\n      throw new Error('Monthly cache limit exceeded');\n    }\n    // Other errors: fallback\n    return await callLLMDirectly(prompt);\n  }\n}\n</code></pre>"},{"location":"guides/error-handling/#pattern-3-circuit-breaker-client-side","title":"Pattern 3: Circuit Breaker Client-Side","text":"<pre><code>class VectorcacheCircuitBreaker {\n  private failureCount = 0;\n  private lastFailureTime = 0;\n  private readonly threshold = 5;\n  private readonly timeout = 60000; // 60 seconds\n\n  async call&lt;T&gt;(fn: () =&gt; Promise&lt;T&gt;): Promise&lt;T&gt; {\n    // If too many recent failures, skip cache\n    if (this.failureCount &gt;= this.threshold) {\n      if (Date.now() - this.lastFailureTime &lt; this.timeout) {\n        throw new Error('Circuit breaker open');\n      }\n      // Reset after timeout\n      this.failureCount = 0;\n    }\n\n    try {\n      const result = await fn();\n      this.failureCount = 0; // Success resets counter\n      return result;\n    } catch (error) {\n      this.failureCount++;\n      this.lastFailureTime = Date.now();\n      throw error;\n    }\n  }\n}\n</code></pre>"},{"location":"guides/error-handling/#next-steps","title":"Next Steps","text":"<ul> <li>SDK Reference - Detailed SDK documentation</li> <li>API Reference - Complete API specification</li> <li>Best Practices - General best practices</li> <li>Security - Security considerations</li> </ul>"},{"location":"guides/security/","title":"Security","text":"<p>Best practices for securing your Vectorcache integration.</p>"},{"location":"guides/security/#api-key-security","title":"API Key Security","text":""},{"location":"guides/security/#store-keys-securely","title":"Store Keys Securely","text":"<p>\u2705 Do: - Use environment variables - Store in secret management systems (AWS Secrets Manager, HashiCorp Vault) - Use <code>.env</code> files (never commit to git)</p> <p>\u274c Don't: - Hardcode in source code - Commit to version control - Share in public channels</p>"},{"location":"guides/security/#rotate-keys-regularly","title":"Rotate Keys Regularly","text":"<p>Implement key rotation:</p> <pre><code>async function rotateAPIKey() {\n  // 1. Create new API key in dashboard\n  const newKey = await createNewKey('Production Key v2');\n\n  // 2. Update environment variable\n  process.env.VECTORCACHE_API_KEY = newKey;\n\n  // 3. Test new key\n  await testAPIKey(newKey);\n\n  // 4. Revoke old key\n  await revokeOldKey(oldKeyId);\n}\n</code></pre>"},{"location":"guides/security/#data-privacy","title":"Data Privacy","text":""},{"location":"guides/security/#sensitive-data-handling","title":"Sensitive Data Handling","text":"<p>Don't cache sensitive information:</p> <pre><code>function shouldCache(prompt: string): boolean {\n  // Don't cache prompts with PII\n  const sensitivePatterns = [\n    /\\b\\d{3}-\\d{2}-\\d{4}\\b/, // SSN\n    /\\b\\d{16}\\b/,            // Credit card\n    /\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b/ // Email\n  ];\n\n  return !sensitivePatterns.some(pattern =&gt; pattern.test(prompt));\n}\n\nif (shouldCache(userPrompt)) {\n  await client.query({ prompt: userPrompt, model: 'gpt-4o' });\n} else {\n  await directLLMCall(userPrompt);\n}\n</code></pre>"},{"location":"guides/security/#data-encryption","title":"Data Encryption","text":"<p>Vectorcache implements comprehensive encryption to protect your data:</p> <ul> <li>In transit: TLS 1.3 for all API communications</li> <li>At rest: Fernet symmetric encryption (AES-128 in CBC mode) for all sensitive cache data</li> <li>Customer prompts are encrypted before storage</li> <li>LLM responses are encrypted before storage</li> <li>Context data is encrypted before storage</li> <li>Embeddings remain unencrypted (required for semantic similarity search)</li> <li>API keys: Encrypted using PBKDF2-derived keys with 100,000 iterations</li> <li>Performance: Encryption adds negligible overhead (~0.05ms per operation, &lt;0.2% of total API latency)</li> <li>Backward compatibility: Automatically handles existing unencrypted cache entries during transition</li> </ul> <p>What's encrypted: - \u2705 Customer prompts (prompt_text) - \u2705 LLM responses (response_text) - \u2705 Context data (context_text) - \u2705 LLM API keys (user_llm_keys) - \u274c Vector embeddings (required for similarity search) - \u274c Metadata and timestamps</p> <p>Encryption method: - Algorithm: Fernet (symmetric encryption) - Key derivation: PBKDF2-HMAC-SHA256 with 100,000 iterations - Cipher: AES-128 in CBC mode with HMAC authentication</p>"},{"location":"guides/security/#access-control","title":"Access Control","text":""},{"location":"guides/security/#environment-separation","title":"Environment Separation","text":"<p>Use separate keys per environment:</p> <pre><code># Development\nVECTORCACHE_API_KEY=vc_dev_...\n\n# Staging\nVECTORCACHE_API_KEY=vc_staging_...\n\n# Production\nVECTORCACHE_API_KEY=vc_prod_...\n</code></pre>"},{"location":"guides/security/#least-privilege","title":"Least Privilege","text":"<p>Grant minimum necessary permissions (coming soon):</p> <pre><code>// Future feature: Scoped API keys\nconst readOnlyKey = createAPIKey({\n  name: 'Analytics Dashboard',\n  scopes: ['cache:read', 'analytics:read']\n});\n</code></pre>"},{"location":"guides/security/#network-security","title":"Network Security","text":""},{"location":"guides/security/#use-https-only","title":"Use HTTPS Only","text":"<p>Always use HTTPS:</p> <pre><code>const client = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!,\n  baseUrl: 'https://api.vectorcache.ai' // Always HTTPS\n});\n</code></pre>"},{"location":"guides/security/#ip-whitelisting-enterprise","title":"IP Whitelisting (Enterprise)","text":"<p>For enterprise customers:</p> <pre><code>// Configure in dashboard\nallowedIPs: [\n  '203.0.113.0/24',\n  '198.51.100.0/24'\n]\n</code></pre>"},{"location":"guides/security/#compliance","title":"Compliance","text":""},{"location":"guides/security/#gdpr-compliance","title":"GDPR Compliance","text":"<ul> <li>Right to be forgotten: Delete user cache entries</li> <li>Data portability: Export user data</li> <li>Consent: Don't cache without user consent</li> </ul>"},{"location":"guides/security/#soc-2-compliance","title":"SOC 2 Compliance","text":"<p>Vectorcache is SOC 2 compliant: - Annual audits - Security monitoring - Incident response procedures</p>"},{"location":"guides/security/#security-checklist","title":"Security Checklist","text":"<ul> <li>[ ] API keys stored in environment variables</li> <li>[ ] <code>.env</code> files in <code>.gitignore</code></li> <li>[ ] Separate keys per environment</li> <li>[ ] Regular key rotation (90 days)</li> <li>[ ] No sensitive data in cache</li> <li>[ ] HTTPS only</li> <li>[ ] Monitor for suspicious activity</li> <li>[ ] Incident response plan in place</li> </ul>"},{"location":"guides/security/#next-steps","title":"Next Steps","text":"<ul> <li>Best Practices - General best practices</li> <li>API Reference - Authentication docs</li> </ul>"},{"location":"guides/similarity-tuning/","title":"Similarity Threshold Tuning","text":"<p>Optimize your cache hit rate by tuning the similarity threshold parameter.</p>"},{"location":"guides/similarity-tuning/#understanding-similarity-threshold","title":"Understanding Similarity Threshold","text":"<p>The <code>similarity_threshold</code> parameter determines how similar two prompts must be for a cache hit. It ranges from 0 to 1:</p> <ul> <li>1.0 = Exact match only</li> <li>0.95-0.99 = Nearly identical</li> <li>0.85-0.94 = Very similar (recommended)</li> <li>0.75-0.84 = Somewhat similar</li> <li>0.0-0.74 = Loosely similar (not recommended)</li> </ul>"},{"location":"guides/similarity-tuning/#how-similarity-works","title":"How Similarity Works","text":"<p>Vectorcache uses cosine similarity between vector embeddings:</p> <pre><code>similarity_score = cosine_similarity(embedding_A, embedding_B)\n\nif similarity_score &gt;= similarity_threshold:\n    return cached_response  # Cache hit\nelse:\n    call_llm()              # Cache miss\n</code></pre>"},{"location":"guides/similarity-tuning/#example-similarity-scores","title":"Example Similarity Scores","text":"<p>Real examples from production:</p> Prompt 1 Prompt 2 Score Match at 0.85? \"What is ML?\" \"What is machine learning?\" 0.94 \u2705 Yes \"Explain AI\" \"What is artificial intelligence?\" 0.88 \u2705 Yes \"Python tutorial\" \"How to learn Python\" 0.82 \u274c No \"Reset password\" \"Change password\" 0.79 \u274c No \"Order status\" \"Track my order\" 0.91 \u2705 Yes"},{"location":"guides/similarity-tuning/#finding-your-optimal-threshold","title":"Finding Your Optimal Threshold","text":""},{"location":"guides/similarity-tuning/#step-1-start-with-default-085","title":"Step 1: Start with Default (0.85)","text":"<p>Begin with the recommended default:</p> <pre><code>const result = await client.query({\n  prompt: userQuery,\n  model: 'gpt-4o',\n  similarityThreshold: 0.85  // Default\n});\n</code></pre>"},{"location":"guides/similarity-tuning/#step-2-monitor-similarity-scores","title":"Step 2: Monitor Similarity Scores","text":"<p>Track actual similarity scores from cache hits:</p> <pre><code>const scores: number[] = [];\n\nasync function trackQuery(prompt: string) {\n  const result = await client.query({\n    prompt,\n    model: 'gpt-4o',\n    similarityThreshold: 0.85\n  });\n\n  if (result.cache_hit &amp;&amp; result.similarity_score) {\n    scores.push(result.similarity_score);\n  }\n\n  return result;\n}\n\n// Analyze scores after 100+ queries\nfunction analyzeScores() {\n  const avg = scores.reduce((a, b) =&gt; a + b, 0) / scores.length;\n  const min = Math.min(...scores);\n  const max = Math.max(...scores);\n\n  console.log(`Average: ${avg.toFixed(3)}`);\n  console.log(`Min: ${min.toFixed(3)}`);\n  console.log(`Max: ${max.toFixed(3)}`);\n}\n</code></pre>"},{"location":"guides/similarity-tuning/#step-3-test-different-thresholds","title":"Step 3: Test Different Thresholds","text":"<p>Run A/B tests with various thresholds:</p> <pre><code>async function testThresholds(prompts: string[]) {\n  const thresholds = [0.75, 0.80, 0.85, 0.90, 0.95];\n\n  for (const threshold of thresholds) {\n    let hits = 0;\n    const scores: number[] = [];\n\n    for (const prompt of prompts) {\n      const result = await client.query({\n        prompt,\n        model: 'gpt-4o',\n        similarityThreshold: threshold\n      });\n\n      if (result.cache_hit) {\n        hits++;\n        if (result.similarity_score) {\n          scores.push(result.similarity_score);\n        }\n      }\n    }\n\n    const hitRate = (hits / prompts.length * 100).toFixed(1);\n    const avgScore = scores.length &gt; 0\n      ? (scores.reduce((a, b) =&gt; a + b, 0) / scores.length).toFixed(3)\n      : 'N/A';\n\n    console.log(`Threshold ${threshold}: ${hitRate}% hits, avg score: ${avgScore}`);\n  }\n}\n</code></pre>"},{"location":"guides/similarity-tuning/#step-4-choose-based-on-use-case","title":"Step 4: Choose Based on Use Case","text":"<p>Select threshold based on your requirements:</p> Use Case Priority Recommended Threshold Legal/Medical Accuracy 0.92-0.95 Financial Accuracy 0.90-0.93 Customer Support Balance 0.85-0.90 Educational Hit Rate 0.80-0.85 FAQs Hit Rate 0.80-0.85 General Content Balance 0.85"},{"location":"guides/similarity-tuning/#use-case-examples","title":"Use Case Examples","text":""},{"location":"guides/similarity-tuning/#high-precision-legal-advice","title":"High Precision: Legal Advice","text":"<p>For legal content, false positives are costly:</p> <pre><code>const legalQuery = await client.query({\n  prompt: 'Interpret clause 5.2 of the agreement',\n  context: 'legal-contract-review',\n  model: 'gpt-4o',\n  similarityThreshold: 0.93  // High threshold for accuracy\n});\n</code></pre> <p>Why 0.93? - Legal queries must be very specific - Different clauses require different interpretations - Cost of wrong answer &gt; cost of LLM call</p>"},{"location":"guides/similarity-tuning/#balanced-customer-support","title":"Balanced: Customer Support","text":"<p>For support chatbots, balance hit rate and accuracy:</p> <pre><code>const supportQuery = await client.query({\n  prompt: 'How do I reset my password?',\n  context: 'customer-support',\n  model: 'gpt-4o',\n  similarityThreshold: 0.87  // Balanced threshold\n});\n</code></pre> <p>Why 0.87? - Similar questions should get same answer - \"Reset password\" vs \"Change password\" should match - Most support queries have common variations</p>"},{"location":"guides/similarity-tuning/#high-hit-rate-educational-faqs","title":"High Hit Rate: Educational FAQs","text":"<p>For educational content, maximize cache hits:</p> <pre><code>const eduQuery = await client.query({\n  prompt: 'What is photosynthesis?',\n  context: 'biology-education',\n  model: 'gpt-4o',\n  similarityThreshold: 0.82  // Lower threshold for more hits\n});\n</code></pre> <p>Why 0.82? - Educational questions have many phrasings - \"What is X?\" vs \"Explain X\" vs \"Define X\" should match - General explanations are reusable</p>"},{"location":"guides/similarity-tuning/#dynamic-threshold-adjustment","title":"Dynamic Threshold Adjustment","text":"<p>Adjust threshold based on context:</p> <pre><code>function getThreshold(context: string): number {\n  const thresholdMap: Record&lt;string, number&gt; = {\n    'legal': 0.93,\n    'medical': 0.92,\n    'financial': 0.90,\n    'support': 0.87,\n    'education': 0.82,\n    'default': 0.85\n  };\n\n  return thresholdMap[context] || thresholdMap['default'];\n}\n\n// Usage\nconst result = await client.query({\n  prompt: userQuery,\n  context: userContext,\n  model: 'gpt-4o',\n  similarityThreshold: getThreshold(userContext)\n});\n</code></pre>"},{"location":"guides/similarity-tuning/#measuring-impact","title":"Measuring Impact","text":""},{"location":"guides/similarity-tuning/#hit-rate-vs-threshold","title":"Hit Rate vs Threshold","text":"<p>Track how threshold affects hit rate:</p> <pre><code>interface ThresholdMetrics {\n  threshold: number;\n  queries: number;\n  hits: number;\n  hitRate: number;\n  avgSimilarity: number;\n  costSaved: number;\n}\n\nasync function measureImpact(\n  prompts: string[],\n  threshold: number\n): Promise&lt;ThresholdMetrics&gt; {\n  let hits = 0;\n  let totalSimilarity = 0;\n  let costSaved = 0;\n\n  for (const prompt of prompts) {\n    const result = await client.query({\n      prompt,\n      model: 'gpt-4o',\n      similarityThreshold: threshold\n    });\n\n    if (result.cache_hit) {\n      hits++;\n      totalSimilarity += result.similarity_score || 0;\n      costSaved += result.cost_saved;\n    }\n  }\n\n  return {\n    threshold,\n    queries: prompts.length,\n    hits,\n    hitRate: hits / prompts.length,\n    avgSimilarity: hits &gt; 0 ? totalSimilarity / hits : 0,\n    costSaved\n  };\n}\n</code></pre>"},{"location":"guides/similarity-tuning/#quality-vs-quantity-trade-off","title":"Quality vs Quantity Trade-off","text":"<p>Higher threshold = Higher quality, Lower hit rate:</p> <pre><code>Threshold 0.95: 15% hit rate, $45/month saved   (high quality)\nThreshold 0.90: 35% hit rate, $105/month saved  (balanced)\nThreshold 0.85: 50% hit rate, $150/month saved  (balanced)\nThreshold 0.80: 65% hit rate, $195/month saved  (quantity)\nThreshold 0.75: 75% hit rate, $225/month saved  (risky)\n</code></pre> <p>Recommendation: Choose the highest threshold that still gives you acceptable hit rate.</p>"},{"location":"guides/similarity-tuning/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"guides/similarity-tuning/#context-specific-thresholds","title":"Context-Specific Thresholds","text":"<p>Use different thresholds for different contexts:</p> <pre><code>const contextThresholds = {\n  'legal-review': 0.93,\n  'medical-advice': 0.92,\n  'customer-support': 0.87,\n  'product-info': 0.85,\n  'general-faq': 0.82\n};\n\nasync function queryWithContextThreshold(\n  prompt: string,\n  context: string\n) {\n  const threshold = contextThresholds[context] || 0.85;\n\n  return await client.query({\n    prompt,\n    context,\n    model: 'gpt-4o',\n    similarityThreshold: threshold\n  });\n}\n</code></pre>"},{"location":"guides/similarity-tuning/#adaptive-thresholds","title":"Adaptive Thresholds","text":"<p>Adjust threshold based on user feedback:</p> <pre><code>class AdaptiveCache {\n  private threshold = 0.85;\n  private feedbackScores: number[] = [];\n\n  async query(prompt: string) {\n    return await client.query({\n      prompt,\n      model: 'gpt-4o',\n      similarityThreshold: this.threshold\n    });\n  }\n\n  recordFeedback(helpful: boolean, similarityScore?: number) {\n    if (!similarityScore) return;\n\n    // If user found it helpful, this score is good\n    if (helpful) {\n      this.feedbackScores.push(similarityScore);\n    }\n\n    // Adjust threshold based on feedback\n    if (this.feedbackScores.length &gt;= 20) {\n      const avgGoodScore = this.feedbackScores.reduce((a, b) =&gt; a + b) /\n                          this.feedbackScores.length;\n\n      // Set threshold slightly below average good score\n      this.threshold = Math.max(0.75, avgGoodScore - 0.05);\n\n      console.log(`Adjusted threshold to ${this.threshold.toFixed(2)}`);\n      this.feedbackScores = []; // Reset for next period\n    }\n  }\n}\n</code></pre>"},{"location":"guides/similarity-tuning/#multi-tier-thresholds","title":"Multi-Tier Thresholds","text":"<p>Try high threshold first, fall back to lower:</p> <pre><code>async function multiTierQuery(prompt: string) {\n  // Try high precision first\n  let result = await client.query({\n    prompt,\n    model: 'gpt-4o',\n    similarityThreshold: 0.92\n  });\n\n  if (result.cache_hit) {\n    return { ...result, tier: 'high-precision' };\n  }\n\n  // Fall back to balanced\n  result = await client.query({\n    prompt,\n    model: 'gpt-4o',\n    similarityThreshold: 0.85\n  });\n\n  return { ...result, tier: result.cache_hit ? 'balanced' : 'miss' };\n}\n</code></pre>"},{"location":"guides/similarity-tuning/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"guides/similarity-tuning/#setting-threshold-too-low","title":"\u274c Setting Threshold Too Low","text":"<pre><code>// DON'T: Too many false positives\nsimilarityThreshold: 0.65  // Will match unrelated queries\n</code></pre> <p>Problem: You'll get irrelevant cached responses</p>"},{"location":"guides/similarity-tuning/#setting-threshold-too-high","title":"\u274c Setting Threshold Too High","text":"<pre><code>// DON'T: Rarely any cache hits\nsimilarityThreshold: 0.98  // Only exact matches\n</code></pre> <p>Problem: Very low hit rate, wasting cache potential</p>"},{"location":"guides/similarity-tuning/#not-testing-with-real-data","title":"\u274c Not Testing with Real Data","text":"<pre><code>// DON'T: Guess based on assumptions\nsimilarityThreshold: 0.85  // \"I think this will work\"\n</code></pre> <p>Solution: Always test with actual user queries</p>"},{"location":"guides/similarity-tuning/#the-right-approach","title":"\u2705 The Right Approach","text":"<pre><code>// DO: Test and measure\nconst optimalThreshold = await findOptimalThreshold(realUserQueries);\n</code></pre>"},{"location":"guides/similarity-tuning/#monitoring-and-alerts","title":"Monitoring and Alerts","text":"<p>Set up alerts for threshold issues:</p> <pre><code>function monitorThreshold(metrics: CacheMetrics) {\n  const hitRate = metrics.cacheHits / metrics.totalQueries;\n  const avgSimilarity = metrics.avgSimilarityScore;\n\n  // Hit rate too low\n  if (hitRate &lt; 0.2 &amp;&amp; metrics.totalQueries &gt; 100) {\n    alert('Low cache hit rate - consider lowering threshold', {\n      hitRate,\n      currentThreshold: 0.85\n    });\n  }\n\n  // Similarity scores too close to threshold\n  if (avgSimilarity &lt; 0.88 &amp;&amp; currentThreshold === 0.85) {\n    alert('Avg similarity close to threshold - may need adjustment', {\n      avgSimilarity,\n      currentThreshold: 0.85\n    });\n  }\n}\n</code></pre>"},{"location":"guides/similarity-tuning/#summary","title":"Summary","text":"<p>Quick Reference:</p> Scenario Recommended Threshold Just starting 0.85 Need high accuracy 0.90-0.95 Want high hit rate 0.80-0.85 Legal/Medical 0.92-0.95 Customer support 0.85-0.90 Education/FAQs 0.80-0.85 <p>Process: 1. Start with 0.85 2. Monitor actual similarity scores 3. Test different thresholds with real data 4. Choose based on your quality vs quantity needs 5. Adjust based on user feedback</p>"},{"location":"guides/similarity-tuning/#next-steps","title":"Next Steps","text":"<ul> <li>Best Practices - Production deployment tips</li> <li>Cost Optimization - Maximize ROI</li> <li>API Reference - Complete API docs</li> </ul>"},{"location":"sdk/curl/","title":"cURL Examples","text":"<p>Use Vectorcache directly with cURL - no SDK required.</p>"},{"location":"sdk/curl/#basic-query","title":"Basic Query","text":"<pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is machine learning?\",\n    \"model\": \"gpt-4o\",\n    \"similarity_threshold\": 0.85\n  }'\n</code></pre>"},{"location":"sdk/curl/#with-context","title":"With Context","text":"<pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Explain photosynthesis\",\n    \"context\": \"educational-biology\",\n    \"model\": \"gpt-4o\",\n    \"similarity_threshold\": 0.85\n  }'\n</code></pre>"},{"location":"sdk/curl/#with-debug-information","title":"With Debug Information","text":"<pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is quantum computing?\",\n    \"model\": \"gpt-4o\",\n    \"similarity_threshold\": 0.85,\n    \"include_debug\": true\n  }'\n</code></pre>"},{"location":"sdk/curl/#using-environment-variables","title":"Using Environment Variables","text":"<p>Store your API key in an environment variable:</p> <pre><code>export VECTORCACHE_API_KEY=\"your_api_key_here\"\n\ncurl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is AI?\",\n    \"model\": \"gpt-4o\"\n  }'\n</code></pre>"},{"location":"sdk/curl/#pretty-print-json-response","title":"Pretty Print JSON Response","text":"<p>Use <code>jq</code> to format the response:</p> <pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is machine learning?\",\n    \"model\": \"gpt-4o\"\n  }' | jq '.'\n</code></pre>"},{"location":"sdk/curl/#save-response-to-file","title":"Save Response to File","text":"<pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"Explain deep learning\",\n    \"model\": \"gpt-4o\"\n  }' -o response.json\n</code></pre>"},{"location":"sdk/curl/#extract-specific-fields","title":"Extract Specific Fields","text":"<p>Extract only the response text:</p> <pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is AI?\",\n    \"model\": \"gpt-4o\"\n  }' | jq -r '.response'\n</code></pre> <p>Check if it was a cache hit:</p> <pre><code>curl -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is AI?\",\n    \"model\": \"gpt-4o\"\n  }' | jq -r '.cache_hit'\n</code></pre>"},{"location":"sdk/curl/#batch-queries-script","title":"Batch Queries Script","text":"<p>Process multiple prompts:</p> <pre><code>#!/bin/bash\n\nPROMPTS=(\n  \"What is machine learning?\"\n  \"Explain deep learning\"\n  \"What are neural networks?\"\n)\n\nfor prompt in \"${PROMPTS[@]}\"; do\n  echo \"Querying: $prompt\"\n  curl -s -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n    -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n      \\\"prompt\\\": \\\"$prompt\\\",\n      \\\"model\\\": \\\"gpt-4o\\\",\n      \\\"similarity_threshold\\\": 0.85\n    }\" | jq '{cache_hit, response: .response[:100]}'\n  echo \"---\"\ndone\n</code></pre>"},{"location":"sdk/curl/#error-handling","title":"Error Handling","text":"<p>Show HTTP status code and response:</p> <pre><code>curl -i -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is AI?\",\n    \"model\": \"gpt-4o\"\n  }'\n</code></pre> <p>Only show status code:</p> <pre><code>curl -s -o /dev/null -w \"%{http_code}\" \\\n  -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is AI?\",\n    \"model\": \"gpt-4o\"\n  }'\n</code></pre>"},{"location":"sdk/curl/#retry-on-failure","title":"Retry on Failure","text":"<p>Simple retry script:</p> <pre><code>#!/bin/bash\n\nMAX_RETRIES=3\nRETRY_DELAY=2\n\nfor i in $(seq 1 $MAX_RETRIES); do\n  response=$(curl -s -w \"\\n%{http_code}\" \\\n    -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n    -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"prompt\": \"What is AI?\",\n      \"model\": \"gpt-4o\"\n    }')\n\n  http_code=$(echo \"$response\" | tail -n1)\n  body=$(echo \"$response\" | head -n-1)\n\n  if [ \"$http_code\" = \"200\" ]; then\n    echo \"$body\" | jq '.'\n    exit 0\n  fi\n\n  echo \"Attempt $i failed with code $http_code\"\n  if [ $i -lt $MAX_RETRIES ]; then\n    echo \"Retrying in ${RETRY_DELAY}s...\"\n    sleep $RETRY_DELAY\n  fi\ndone\n\necho \"Max retries exceeded\"\nexit 1\n</code></pre>"},{"location":"sdk/curl/#testing-different-thresholds","title":"Testing Different Thresholds","text":"<p>Test various similarity thresholds:</p> <pre><code>#!/bin/bash\n\nTHRESHOLDS=(0.70 0.80 0.85 0.90 0.95)\n\nfor threshold in \"${THRESHOLDS[@]}\"; do\n  echo \"Testing threshold: $threshold\"\n  curl -s -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n    -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n      \\\"prompt\\\": \\\"What is machine learning?\\\",\n      \\\"model\\\": \\\"gpt-4o\\\",\n      \\\"similarity_threshold\\\": $threshold\n    }\" | jq '{threshold: '\"$threshold\"', cache_hit, similarity_score}'\n  echo \"---\"\ndone\n</code></pre>"},{"location":"sdk/curl/#response-format","title":"Response Format","text":""},{"location":"sdk/curl/#success-response-cache-hit","title":"Success Response (Cache Hit)","text":"<pre><code>{\n  \"cache_hit\": true,\n  \"response\": \"Machine learning is a subset of artificial intelligence...\",\n  \"similarity_score\": 0.92,\n  \"cost_saved\": 0.003,\n  \"llm_provider\": \"cache\"\n}\n</code></pre>"},{"location":"sdk/curl/#success-response-cache-miss","title":"Success Response (Cache Miss)","text":"<pre><code>{\n  \"cache_hit\": false,\n  \"response\": \"Machine learning is a subset of artificial intelligence...\",\n  \"similarity_score\": null,\n  \"cost_saved\": 0,\n  \"llm_provider\": \"openai\"\n}\n</code></pre>"},{"location":"sdk/curl/#error-response","title":"Error Response","text":"<pre><code>{\n  \"detail\": \"Invalid API key\"\n}\n</code></pre>"},{"location":"sdk/curl/#common-http-status-codes","title":"Common HTTP Status Codes","text":"Code Meaning 200 Success 400 Bad Request - Invalid parameters 401 Unauthorized - Invalid API key 429 Too Many Requests - Rate limited 500 Internal Server Error"},{"location":"sdk/curl/#advanced-usage","title":"Advanced Usage","text":""},{"location":"sdk/curl/#measure-response-time","title":"Measure Response Time","text":"<pre><code>curl -w \"\\nTime: %{time_total}s\\n\" \\\n  -X POST \"https://api.vectorcache.ai/v1/cache/query\" \\\n  -H \"Authorization: Bearer $VECTORCACHE_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"prompt\": \"What is AI?\",\n    \"model\": \"gpt-4o\"\n  }' | jq '.'\n</code></pre>"},{"location":"sdk/curl/#parallel-requests","title":"Parallel Requests","text":"<p>Use GNU Parallel to send concurrent requests:</p> <pre><code>parallel -j 5 \"curl -s -X POST 'https://api.vectorcache.ai/v1/cache/query' \\\n  -H 'Authorization: Bearer $VECTORCACHE_API_KEY' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\\\"prompt\\\": \\\"What is AI?\\\", \\\"model\\\": \\\"gpt-4o\\\"}' | jq '.cache_hit'\" \\\n  ::: {1..10}\n</code></pre>"},{"location":"sdk/curl/#next-steps","title":"Next Steps","text":"<ul> <li>JavaScript SDK - Full SDK with TypeScript support</li> <li>Python SDK - Python implementation</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"sdk/javascript/","title":"JavaScript/TypeScript SDK","text":"<p>Complete guide to the Vectorcache JavaScript/TypeScript SDK.</p>"},{"location":"sdk/javascript/#installation","title":"Installation","text":"<pre><code>npm install vectorcache\n</code></pre>"},{"location":"sdk/javascript/#quick-start","title":"Quick Start","text":"<pre><code>import { VectorcacheClient } from 'vectorcache';\n\nconst client = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!,\n  projectId: process.env.VECTORCACHE_PROJECT_ID!,\n});\n\nconst result = await client.query({\n  prompt: 'What is machine learning?',\n  model: 'gpt-4o',\n  similarityThreshold: 0.85\n});\n\nconsole.log(result.response);\n</code></pre>"},{"location":"sdk/javascript/#api-reference","title":"API Reference","text":""},{"location":"sdk/javascript/#vectorcacheclient","title":"VectorcacheClient","text":""},{"location":"sdk/javascript/#constructor","title":"Constructor","text":"<pre><code>new VectorcacheClient(config: VectorcacheConfig)\n</code></pre> <p>Parameters:</p> Name Type Required Default Description <code>config.apiKey</code> string Yes - Your Vectorcache API key <code>config.projectId</code> string Yes - Your Vectorcache project ID <code>config.baseUrl</code> string No <code>https://api.vectorcache.ai</code> API base URL <code>config.timeout</code> number No <code>30000</code> Request timeout (ms)"},{"location":"sdk/javascript/#methods","title":"Methods","text":""},{"location":"sdk/javascript/#query","title":"<code>query()</code>","text":"<p>Query the semantic cache.</p> <pre><code>async query(request: CacheQueryRequest): Promise&lt;CacheQueryResponse&gt;\n</code></pre> <p>Request Parameters:</p> <pre><code>interface CacheQueryRequest {\n  prompt: string;                    // Required: The prompt to cache/query\n  model: string;                     // Required: LLM model (e.g., 'gpt-4o')\n  context?: string;                  // Optional: Additional context\n  similarityThreshold?: number;      // Optional: 0-1, default 0.85\n  includeDebug?: boolean;           // Optional: Include debug info\n}\n</code></pre> <p>Response:</p> <pre><code>interface CacheQueryResponse {\n  cache_hit: boolean;               // Whether the query hit the cache\n  response: string;                 // The LLM response\n  similarity_score: number | null;  // Similarity score (null on cache miss)\n  cost_saved: number;              // Cost saved in USD (0 on cache miss)\n  llm_provider: string;            // Provider used ('cache' or LLM name)\n  debug?: DebugInfo;               // Debug information (if requested)\n}\n</code></pre> <p>Example:</p> <pre><code>const result = await client.query({\n  prompt: 'Explain quantum computing',\n  context: 'Educational content',\n  model: 'gpt-4o',\n  similarityThreshold: 0.85,\n  includeDebug: true\n});\n\nif (result.cache_hit) {\n  console.log(`Cache hit! Similarity: ${result.similarity_score}`);\n  console.log(`Cost saved: $${result.cost_saved}`);\n} else {\n  console.log('Cache miss - fetched from LLM');\n}\n</code></pre>"},{"location":"sdk/javascript/#typescript-support","title":"TypeScript Support","text":"<p>The SDK is written in TypeScript and includes full type definitions.</p>"},{"location":"sdk/javascript/#import-types","title":"Import Types","text":"<pre><code>import type {\n  VectorcacheClient,\n  VectorcacheConfig,\n  CacheQueryRequest,\n  CacheQueryResponse,\n  VectorcacheError\n} from 'vectorcache';\n</code></pre>"},{"location":"sdk/javascript/#type-safe-queries","title":"Type-Safe Queries","text":"<pre><code>const request: CacheQueryRequest = {\n  prompt: 'What is AI?',\n  model: 'gpt-4o',\n  similarityThreshold: 0.85\n};\n\nconst response: CacheQueryResponse = await client.query(request);\n</code></pre>"},{"location":"sdk/javascript/#error-handling","title":"Error Handling","text":""},{"location":"sdk/javascript/#error-types","title":"Error Types","text":"<p>The SDK throws typed errors:</p> <pre><code>class VectorcacheError extends Error {\n  statusCode?: number;\n  response?: any;\n}\n</code></pre>"},{"location":"sdk/javascript/#handling-errors","title":"Handling Errors","text":"<pre><code>import { VectorcacheError } from 'vectorcache';\n\ntry {\n  const result = await client.query({\n    prompt: 'What is AI?',\n    model: 'gpt-4o'\n  });\n} catch (error) {\n  if (error instanceof VectorcacheError) {\n    switch (error.statusCode) {\n      case 401:\n        console.error('Authentication failed - check your API key');\n        break;\n      case 429:\n        console.error('Rate limit exceeded');\n        break;\n      case 500:\n        console.error('Server error:', error.message);\n        break;\n      default:\n        console.error('Unexpected error:', error);\n    }\n  } else {\n    console.error('Network or unknown error:', error);\n  }\n}\n</code></pre>"},{"location":"sdk/javascript/#retry-logic","title":"Retry Logic","text":"<p>Implement retry logic for transient failures:</p> <pre><code>async function queryWithRetry(\n  client: VectorcacheClient,\n  request: CacheQueryRequest,\n  maxRetries = 3\n): Promise&lt;CacheQueryResponse&gt; {\n  for (let attempt = 1; attempt &lt;= maxRetries; attempt++) {\n    try {\n      return await client.query(request);\n    } catch (error) {\n      if (error instanceof VectorcacheError &amp;&amp; error.statusCode === 429) {\n        // Rate limit - wait before retry\n        const delay = Math.min(1000 * Math.pow(2, attempt), 10000);\n        await new Promise(resolve =&gt; setTimeout(resolve, delay));\n        continue;\n      }\n      throw error; // Don't retry other errors\n    }\n  }\n  throw new Error('Max retries exceeded');\n}\n</code></pre>"},{"location":"sdk/javascript/#advanced-usage","title":"Advanced Usage","text":""},{"location":"sdk/javascript/#context-aware-caching","title":"Context-Aware Caching","text":"<p>Use context to segment your cache:</p> <pre><code>// Educational queries\nconst eduResult = await client.query({\n  prompt: 'What is photosynthesis?',\n  context: 'biology-education',\n  model: 'gpt-4o'\n});\n\n// Technical documentation\nconst docResult = await client.query({\n  prompt: 'What is photosynthesis?',\n  context: 'technical-documentation',\n  model: 'gpt-4o'\n});\n</code></pre> <p>These will be cached separately even though the prompt is the same.</p>"},{"location":"sdk/javascript/#multiple-clients","title":"Multiple Clients","text":"<p>Create separate clients for different projects:</p> <pre><code>const productionClient = new VectorcacheClient({\n  apiKey: process.env.PROD_VECTORCACHE_KEY!,\n});\n\nconst stagingClient = new VectorcacheClient({\n  apiKey: process.env.STAGING_VECTORCACHE_KEY!,\n  baseUrl: 'https://staging-api.vectorcache.ai'\n});\n</code></pre>"},{"location":"sdk/javascript/#streaming-responses-coming-soon","title":"Streaming Responses (Coming Soon)","text":"<pre><code>// Future feature\nconst stream = await client.queryStream({\n  prompt: 'Long explanation...',\n  model: 'gpt-4o'\n});\n\nfor await (const chunk of stream) {\n  process.stdout.write(chunk);\n}\n</code></pre>"},{"location":"sdk/javascript/#framework-integration","title":"Framework Integration","text":""},{"location":"sdk/javascript/#nextjs","title":"Next.js","text":"<pre><code>// app/api/chat/route.ts\nimport { VectorcacheClient } from 'vectorcache';\nimport { NextRequest, NextResponse } from 'next/server';\n\nconst client = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!,\n});\n\nexport async function POST(request: NextRequest) {\n  const { prompt } = await request.json();\n\n  const result = await client.query({\n    prompt,\n    model: 'gpt-4o',\n    similarityThreshold: 0.85\n  });\n\n  return NextResponse.json(result);\n}\n</code></pre>"},{"location":"sdk/javascript/#express","title":"Express","text":"<pre><code>import express from 'express';\nimport { VectorcacheClient } from 'vectorcache';\n\nconst app = express();\nconst client = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!,\n});\n\napp.post('/api/query', async (req, res) =&gt; {\n  try {\n    const result = await client.query({\n      prompt: req.body.prompt,\n      model: 'gpt-4o'\n    });\n    res.json(result);\n  } catch (error) {\n    res.status(500).json({ error: error.message });\n  }\n});\n</code></pre>"},{"location":"sdk/javascript/#react-hook","title":"React Hook","text":"<pre><code>import { useState } from 'react';\nimport { VectorcacheClient } from 'vectorcache';\n\nconst client = new VectorcacheClient({\n  apiKey: process.env.NEXT_PUBLIC_VECTORCACHE_KEY!,\n});\n\nexport function useVectorcache() {\n  const [loading, setLoading] = useState(false);\n  const [error, setError] = useState&lt;Error | null&gt;(null);\n\n  const query = async (prompt: string, model: string) =&gt; {\n    setLoading(true);\n    setError(null);\n    try {\n      const result = await client.query({ prompt, model });\n      return result;\n    } catch (err) {\n      setError(err as Error);\n      throw err;\n    } finally {\n      setLoading(false);\n    }\n  };\n\n  return { query, loading, error };\n}\n</code></pre>"},{"location":"sdk/javascript/#examples","title":"Examples","text":""},{"location":"sdk/javascript/#chatbot-with-cache-metrics","title":"Chatbot with Cache Metrics","text":"<pre><code>import { VectorcacheClient } from 'vectorcache';\n\nconst client = new VectorcacheClient({\n  apiKey: process.env.VECTORCACHE_API_KEY!,\n});\n\nlet totalQueries = 0;\nlet cacheHits = 0;\nlet totalCostSaved = 0;\n\nasync function chat(userMessage: string): Promise&lt;string&gt; {\n  const result = await client.query({\n    prompt: userMessage,\n    context: 'customer-support-bot',\n    model: 'gpt-4o',\n    similarityThreshold: 0.85\n  });\n\n  totalQueries++;\n  if (result.cache_hit) {\n    cacheHits++;\n    totalCostSaved += result.cost_saved;\n  }\n\n  console.log(`Cache hit rate: ${(cacheHits / totalQueries * 100).toFixed(1)}%`);\n  console.log(`Total cost saved: $${totalCostSaved.toFixed(4)}`);\n\n  return result.response;\n}\n</code></pre>"},{"location":"sdk/javascript/#batch-processing","title":"Batch Processing","text":"<pre><code>async function processBatch(prompts: string[]) {\n  const results = await Promise.all(\n    prompts.map(prompt =&gt;\n      client.query({\n        prompt,\n        model: 'gpt-4o',\n        similarityThreshold: 0.85\n      })\n    )\n  );\n\n  const hitRate = results.filter(r =&gt; r.cache_hit).length / results.length;\n  const totalSaved = results.reduce((sum, r) =&gt; sum + r.cost_saved, 0);\n\n  console.log(`Batch hit rate: ${(hitRate * 100).toFixed(1)}%`);\n  console.log(`Batch cost saved: $${totalSaved.toFixed(4)}`);\n\n  return results;\n}\n</code></pre>"},{"location":"sdk/javascript/#best-practices","title":"Best Practices","text":"<ol> <li>Store API keys securely - Use environment variables</li> <li>Handle errors gracefully - Implement proper error handling</li> <li>Set appropriate timeouts - Based on your use case</li> <li>Monitor cache performance - Track hit rates and cost savings</li> <li>Use context wisely - Segment caches by use case</li> <li>Test similarity thresholds - Find optimal values for your data</li> </ol>"},{"location":"sdk/javascript/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide</li> <li>API Reference</li> <li>Best Practices</li> </ul>"},{"location":"sdk/python/","title":"Python SDK","text":"<p>Complete guide to using Vectorcache with Python (REST API approach).</p> <p>Python SDK Coming Soon</p> <p>A dedicated Python SDK package is in development. Currently, use the REST API with <code>requests</code> or <code>httpx</code>.</p>"},{"location":"sdk/python/#installation","title":"Installation","text":"<pre><code>pip install requests\n# or for async support\npip install httpx\n</code></pre>"},{"location":"sdk/python/#quick-start","title":"Quick Start","text":"<pre><code>import requests\nimport os\n\napi_key = os.environ.get('VECTORCACHE_API_KEY')\nbase_url = 'https://api.vectorcache.ai'\n\nheaders = {\n    'Authorization': f'Bearer {api_key}',\n    'Content-Type': 'application/json'\n}\n\ndata = {\n    'prompt': 'What is machine learning?',\n    'model': 'gpt-4o',\n    'similarity_threshold': 0.85\n}\n\nresponse = requests.post(\n    f'{base_url}/v1/cache/query',\n    json=data,\n    headers=headers\n)\n\nresult = response.json()\nprint(f\"Response: {result['response']}\")\nprint(f\"Cache hit: {result['cache_hit']}\")\n</code></pre>"},{"location":"sdk/python/#using-requests","title":"Using requests","text":""},{"location":"sdk/python/#basic-query","title":"Basic Query","text":"<pre><code>import requests\nfrom typing import Dict, Any\n\ndef query_vectorcache(\n    prompt: str,\n    model: str,\n    api_key: str,\n    similarity_threshold: float = 0.85,\n    context: str = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Query the Vectorcache API.\"\"\"\n\n    url = 'https://api.vectorcache.ai/v1/cache/query'\n\n    headers = {\n        'Authorization': f'Bearer {api_key}',\n        'Content-Type': 'application/json'\n    }\n\n    data = {\n        'prompt': prompt,\n        'model': model,\n        'similarity_threshold': similarity_threshold\n    }\n\n    if context:\n        data['context'] = context\n\n    response = requests.post(url, json=data, headers=headers)\n    response.raise_for_status()\n\n    return response.json()\n\n# Usage\nresult = query_vectorcache(\n    prompt='Explain quantum computing',\n    model='gpt-4o',\n    api_key=os.environ['VECTORCACHE_API_KEY']\n)\n\nprint(result)\n</code></pre>"},{"location":"sdk/python/#error-handling","title":"Error Handling","text":"<pre><code>import requests\n\ndef query_with_error_handling(prompt: str, model: str, api_key: str):\n    url = 'https://api.vectorcache.ai/v1/cache/query'\n\n    headers = {\n        'Authorization': f'Bearer {api_key}',\n        'Content-Type': 'application/json'\n    }\n\n    data = {\n        'prompt': prompt,\n        'model': model,\n        'similarity_threshold': 0.85\n    }\n\n    try:\n        response = requests.post(url, json=data, headers=headers, timeout=30)\n        response.raise_for_status()\n        return response.json()\n\n    except requests.exceptions.HTTPError as e:\n        if e.response.status_code == 401:\n            raise ValueError(\"Invalid API key\")\n        elif e.response.status_code == 429:\n            raise ValueError(\"Rate limit exceeded\")\n        elif e.response.status_code == 500:\n            raise ValueError(f\"Server error: {e.response.text}\")\n        else:\n            raise ValueError(f\"HTTP error: {e}\")\n\n    except requests.exceptions.Timeout:\n        raise ValueError(\"Request timed out\")\n\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Request failed: {e}\")\n</code></pre>"},{"location":"sdk/python/#using-httpx-async","title":"Using httpx (Async)","text":""},{"location":"sdk/python/#async-client","title":"Async Client","text":"<pre><code>import httpx\nimport asyncio\nfrom typing import Dict, Any\n\nasync def query_vectorcache_async(\n    prompt: str,\n    model: str,\n    api_key: str,\n    similarity_threshold: float = 0.85\n) -&gt; Dict[str, Any]:\n    \"\"\"Async query to Vectorcache API.\"\"\"\n\n    url = 'https://api.vectorcache.ai/v1/cache/query'\n\n    headers = {\n        'Authorization': f'Bearer {api_key}',\n        'Content-Type': 'application/json'\n    }\n\n    data = {\n        'prompt': prompt,\n        'model': model,\n        'similarity_threshold': similarity_threshold\n    }\n\n    async with httpx.AsyncClient(timeout=30.0) as client:\n        response = await client.post(url, json=data, headers=headers)\n        response.raise_for_status()\n        return response.json()\n\n# Usage\nasync def main():\n    result = await query_vectorcache_async(\n        prompt='What is AI?',\n        model='gpt-4o',\n        api_key=os.environ['VECTORCACHE_API_KEY']\n    )\n    print(result)\n\nasyncio.run(main())\n</code></pre>"},{"location":"sdk/python/#batch-processing","title":"Batch Processing","text":"<pre><code>import httpx\nimport asyncio\nfrom typing import List, Dict, Any\n\nasync def process_batch(\n    prompts: List[str],\n    model: str,\n    api_key: str\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"Process multiple prompts concurrently.\"\"\"\n\n    async with httpx.AsyncClient(timeout=30.0) as client:\n        tasks = [\n            query_with_client(client, prompt, model, api_key)\n            for prompt in prompts\n        ]\n        return await asyncio.gather(*tasks)\n\nasync def query_with_client(\n    client: httpx.AsyncClient,\n    prompt: str,\n    model: str,\n    api_key: str\n) -&gt; Dict[str, Any]:\n    url = 'https://api.vectorcache.ai/v1/cache/query'\n\n    headers = {\n        'Authorization': f'Bearer {api_key}',\n        'Content-Type': 'application/json'\n    }\n\n    data = {\n        'prompt': prompt,\n        'model': model,\n        'similarity_threshold': 0.85\n    }\n\n    response = await client.post(url, json=data, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\n# Usage\nasync def main():\n    prompts = [\n        'What is machine learning?',\n        'Explain deep learning',\n        'What are neural networks?'\n    ]\n\n    results = await process_batch(\n        prompts,\n        model='gpt-4o',\n        api_key=os.environ['VECTORCACHE_API_KEY']\n    )\n\n    cache_hits = sum(1 for r in results if r['cache_hit'])\n    print(f\"Cache hit rate: {cache_hits / len(results) * 100:.1f}%\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"sdk/python/#vectorcache-client-class","title":"Vectorcache Client Class","text":"<p>Create a reusable client class:</p> <pre><code>import requests\nfrom typing import Optional, Dict, Any\nfrom dataclasses import dataclass\n\n@dataclass\nclass VectorcacheResponse:\n    cache_hit: bool\n    response: str\n    similarity_score: Optional[float]\n    cost_saved: float\n    llm_provider: str\n\nclass VectorcacheClient:\n    def __init__(self, api_key: str, base_url: str = 'https://api.vectorcache.ai'):\n        self.api_key = api_key\n        self.base_url = base_url\n        self.session = requests.Session()\n        self.session.headers.update({\n            'Authorization': f'Bearer {api_key}',\n            'Content-Type': 'application/json'\n        })\n\n    def query(\n        self,\n        prompt: str,\n        model: str,\n        similarity_threshold: float = 0.85,\n        context: Optional[str] = None,\n        include_debug: bool = False\n    ) -&gt; VectorcacheResponse:\n        \"\"\"Query the cache.\"\"\"\n\n        url = f'{self.base_url}/v1/cache/query'\n\n        data = {\n            'prompt': prompt,\n            'model': model,\n            'similarity_threshold': similarity_threshold,\n            'include_debug': include_debug\n        }\n\n        if context:\n            data['context'] = context\n\n        response = self.session.post(url, json=data, timeout=30)\n        response.raise_for_status()\n\n        result = response.json()\n        return VectorcacheResponse(\n            cache_hit=result['cache_hit'],\n            response=result['response'],\n            similarity_score=result.get('similarity_score'),\n            cost_saved=result['cost_saved'],\n            llm_provider=result['llm_provider']\n        )\n\n    def close(self):\n        \"\"\"Close the session.\"\"\"\n        self.session.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n\n# Usage\nwith VectorcacheClient(api_key=os.environ['VECTORCACHE_API_KEY']) as client:\n    result = client.query(\n        prompt='What is machine learning?',\n        model='gpt-4o',\n        similarity_threshold=0.85\n    )\n\n    print(f\"Response: {result.response}\")\n    print(f\"Cache hit: {result.cache_hit}\")\n    if result.cache_hit:\n        print(f\"Similarity: {result.similarity_score}\")\n        print(f\"Saved: ${result.cost_saved}\")\n</code></pre>"},{"location":"sdk/python/#framework-integration","title":"Framework Integration","text":""},{"location":"sdk/python/#fastapi","title":"FastAPI","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport os\n\napp = FastAPI()\nclient = VectorcacheClient(api_key=os.environ['VECTORCACHE_API_KEY'])\n\nclass QueryRequest(BaseModel):\n    prompt: str\n    model: str = 'gpt-4o'\n    similarity_threshold: float = 0.85\n\n@app.post(\"/query\")\nasync def query_cache(request: QueryRequest):\n    try:\n        result = client.query(\n            prompt=request.prompt,\n            model=request.model,\n            similarity_threshold=request.similarity_threshold\n        )\n        return result\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n</code></pre>"},{"location":"sdk/python/#django","title":"Django","text":"<pre><code># views.py\nfrom django.http import JsonResponse\nfrom django.views import View\nimport json\nimport os\n\nclass CacheQueryView(View):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.client = VectorcacheClient(\n            api_key=os.environ['VECTORCACHE_API_KEY']\n        )\n\n    def post(self, request):\n        try:\n            data = json.loads(request.body)\n            result = self.client.query(\n                prompt=data['prompt'],\n                model=data.get('model', 'gpt-4o'),\n                similarity_threshold=data.get('similarity_threshold', 0.85)\n            )\n            return JsonResponse({\n                'cache_hit': result.cache_hit,\n                'response': result.response,\n                'cost_saved': result.cost_saved\n            })\n        except Exception as e:\n            return JsonResponse({'error': str(e)}, status=500)\n</code></pre>"},{"location":"sdk/python/#flask","title":"Flask","text":"<pre><code>from flask import Flask, request, jsonify\nimport os\n\napp = Flask(__name__)\nclient = VectorcacheClient(api_key=os.environ['VECTORCACHE_API_KEY'])\n\n@app.route('/query', methods=['POST'])\ndef query():\n    try:\n        data = request.json\n        result = client.query(\n            prompt=data['prompt'],\n            model=data.get('model', 'gpt-4o'),\n            similarity_threshold=data.get('similarity_threshold', 0.85)\n        )\n        return jsonify({\n            'cache_hit': result.cache_hit,\n            'response': result.response,\n            'cost_saved': result.cost_saved\n        })\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n</code></pre>"},{"location":"sdk/python/#examples","title":"Examples","text":""},{"location":"sdk/python/#chatbot-with-metrics","title":"Chatbot with Metrics","text":"<pre><code>class ChatbotWithCache:\n    def __init__(self, api_key: str):\n        self.client = VectorcacheClient(api_key)\n        self.total_queries = 0\n        self.cache_hits = 0\n        self.total_saved = 0.0\n\n    def chat(self, message: str) -&gt; str:\n        result = self.client.query(\n            prompt=message,\n            model='gpt-4o',\n            context='customer-support',\n            similarity_threshold=0.85\n        )\n\n        self.total_queries += 1\n        if result.cache_hit:\n            self.cache_hits += 1\n            self.total_saved += result.cost_saved\n\n        return result.response\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        hit_rate = (self.cache_hits / self.total_queries * 100\n                   if self.total_queries &gt; 0 else 0)\n        return {\n            'total_queries': self.total_queries,\n            'cache_hits': self.cache_hits,\n            'hit_rate': f'{hit_rate:.1f}%',\n            'total_saved': f'${self.total_saved:.4f}'\n        }\n\n# Usage\nbot = ChatbotWithCache(api_key=os.environ['VECTORCACHE_API_KEY'])\n\nprint(bot.chat('What is machine learning?'))\nprint(bot.chat('Explain ML'))  # Similar query, likely cache hit\nprint(bot.get_stats())\n</code></pre>"},{"location":"sdk/python/#retry-logic","title":"Retry Logic","text":"<pre><code>import time\nfrom typing import Optional\n\ndef query_with_retry(\n    client: VectorcacheClient,\n    prompt: str,\n    model: str,\n    max_retries: int = 3,\n    backoff_factor: float = 2.0\n) -&gt; Optional[VectorcacheResponse]:\n    \"\"\"Query with exponential backoff retry.\"\"\"\n\n    for attempt in range(max_retries):\n        try:\n            return client.query(prompt=prompt, model=model)\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == 429:  # Rate limit\n                if attempt &lt; max_retries - 1:\n                    delay = min(backoff_factor ** attempt, 10)\n                    print(f\"Rate limited, retrying in {delay}s...\")\n                    time.sleep(delay)\n                    continue\n            raise\n        except requests.exceptions.Timeout:\n            if attempt &lt; max_retries - 1:\n                print(f\"Timeout, retrying... (attempt {attempt + 1})\")\n                continue\n            raise\n\n    return None\n</code></pre>"},{"location":"sdk/python/#best-practices","title":"Best Practices","text":"<ol> <li>Use environment variables for API keys</li> <li>Implement proper error handling with try/except blocks</li> <li>Set timeouts on all requests (default: 30 seconds)</li> <li>Use connection pooling with <code>requests.Session()</code></li> <li>Implement retry logic for transient failures</li> <li>Monitor performance - track cache hit rates</li> <li>Use async (<code>httpx</code>) for high-throughput applications</li> </ol>"},{"location":"sdk/python/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide</li> <li>API Reference</li> <li>Best Practices</li> </ul>"}]}